{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stage1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFejSxtjjbBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "cdc8e33b-a910-48ca-c1bc-c8537c4f4c2e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/3wG_58vxP_oXc5JxA-KqNBN3x12Yz7sBiDj11-GnrGSMb3bIUCVZGu8\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8dbbiaMfTvy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "8d13f6a7-4592-4b00-ae87-ba92f367ee00"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Sep 10 04:38:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfYi6FN-jfE3"
      },
      "source": [
        "! mkdir /root/.kaggle\n",
        "! cp '/content/drive/My Drive/kaggle.json' /root/.kaggle\n",
        "! chmod 400 /root/.kaggle/kaggle.json\n",
        "\n",
        "!pip uninstall -y kaggle >> quit\n",
        "!pip install --upgrade pip >> quit\n",
        "!pip install kaggle==1.5.6 >> quit\n",
        "!kaggle -v >> quit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2CDVTjpfnED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3cf1a0a3-c752-400c-9b81-38c372feb6af"
      },
      "source": [
        "%%time\n",
        "!kaggle datasets download -d gopidurgaprasad/birdsong-stage1-1sec\n",
        "!unzip birdsong-stage1-1sec.zip >> quit\n",
        "!rm birdsong-stage1-1sec.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading birdsong-stage1-1sec.zip to /content\n",
            "100% 8.67G/8.68G [03:19<00:00, 39.5MB/s]\n",
            "100% 8.68G/8.68G [03:19<00:00, 46.7MB/s]\n",
            "CPU times: user 1.43 s, sys: 362 ms, total: 1.8 s\n",
            "Wall time: 8min 44s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ4BTJYVcCwz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "be270138-8a57-4402-be06-feec3544326e"
      },
      "source": [
        "%%time\n",
        "!kaggle datasets download -d gopidurgaprasad/birdsong-stage1-1sec-sudo-5\n",
        "!unzip birdsong-stage1-1sec-sudo-5.zip >> quit\n",
        "!rm birdsong-stage1-1sec-sudo-5.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading birdsong-stage1-1sec-sudo-5.zip to /content\n",
            "100% 3.26G/3.26G [00:55<00:00, 42.3MB/s]\n",
            "100% 3.26G/3.26G [00:55<00:00, 63.0MB/s]\n",
            "CPU times: user 408 ms, sys: 122 ms, total: 530 ms\n",
            "Wall time: 2min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbKf9TZUQV0L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6e275154-54ea-4a99-c08f-30888f4c8492"
      },
      "source": [
        "%%time\n",
        "!kaggle datasets download -d gopidurgaprasad/birdsong-stage1-cv\n",
        "!unzip birdsong-stage1-cv.zip >> quit\n",
        "!rm birdsong-stage1-cv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading birdsong-stage1-cv.zip to /content\n",
            "100% 1.83G/1.83G [00:38<00:00, 26.3MB/s]\n",
            "100% 1.83G/1.83G [00:38<00:00, 51.5MB/s]\n",
            "CPU times: user 390 ms, sys: 109 ms, total: 499 ms\n",
            "Wall time: 1min 42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLYj5alKYsAR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8169d3d1-6ec4-4f4c-d9d1-31e177af0618"
      },
      "source": [
        "!kaggle datasets download -d gopidurgaprasad/birdsong-stage1-cv2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading birdsong-stage1-cv2.zip to /content\n",
            "100% 1.78G/1.78G [00:13<00:00, 143MB/s]\n",
            "100% 1.78G/1.78G [00:13<00:00, 144MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aZj39O6Yv2H"
      },
      "source": [
        "!unzip birdsong-stage1-cv2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCC3TZLdRw2E"
      },
      "source": [
        "!pip install soundfile >> quit\n",
        "!pip install catalyst >> quit\n",
        "!pip install torchaudio >> quit\n",
        "!pip install torchlibrosa >> quit\n",
        "!pip install timm >> quit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPj5S2Jp4at4"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPJCMjJpfj9B"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "#train_wav0 = glob.glob(\"tmp/birdsongWav/train_wav_00/*/*.wav\")\n",
        "#train_wav1 = glob.glob(\"tmp/birdsongWav/train_wav_01/*/*.wav\")\n",
        "#train_wav2 = glob.glob(\"tmp/birdsongWav/train_wav_02/*/*.wav\")\n",
        "#train_wav3 = glob.glob(\"tmp/birdsongWav/train_wav_03/*/*.wav\")\n",
        "#train_wav4 = glob.glob(\"tmp/birdsongWav/train_wav_04/*/*.wav\")\n",
        "\n",
        "train_wav1 = glob.glob(\"/content/train_stage1_1sec/tmp/birdsongWav/train_stage1_1sec/*.wav\") \n",
        "train_wav2 = glob.glob(\"/content/train_stage1_1sec_sudo/tmp/birdsongWav/train_stage1_1sec_sudo/*.wav\")\n",
        "\n",
        "train_wav = train_wav1 + train_wav2\n",
        "\n",
        "train_wav_df = pd.DataFrame()\n",
        "train_wav_df[\"path\"] = train_wav\n",
        "train_wav_df[\"filename\"] = train_wav_df[\"path\"].apply(lambda x: x.split('/')[-1].strip('.wav'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u6f7dyxV3gj"
      },
      "source": [
        "stage1_df = pd.read_csv(\"stage1_df.csv\")\n",
        "stage1_df[\"filename\"] = stage1_df[\"file_name\"].apply(lambda x: x.split('/')[-1].strip('.wav'))\n",
        "\n",
        "stage1_df = stage1_df.merge(train_wav_df, on=\"filename\", how=\"left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNs9J3THV6Az"
      },
      "source": [
        "stage1_df.to_csv(\"drive/My Drive/Cornell Birdcall Identification/input/train_stage1_df.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAJ7RwT7WTHS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7OyKongWSw6"
      },
      "source": [
        "valid_wav1 = glob.glob(\"/content/tmp/birdsongWav/stage1_cv2/*.wav\") \n",
        "\n",
        "valid_wav = valid_wav1 #+ train_wav2\n",
        "\n",
        "valid_wav_df = pd.DataFrame()\n",
        "valid_wav_df[\"path\"] = valid_wav\n",
        "valid_wav_df[\"filename\"] = valid_wav_df[\"path\"].apply(lambda x: x.split('/')[-1].split('.')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOCGz4efYN74"
      },
      "source": [
        "valid_wav_df[\"pred_code\"] = valid_wav_df[\"filename\"].apply(lambda x: x.split('_')[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9gmGkaYn4a"
      },
      "source": [
        "valid_wav_df[\"pred_code\"] = valid_wav_df[\"pred_code\"].apply(lambda x: \" \".join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwltZ5lFWScj"
      },
      "source": [
        "valid_wav_df.to_csv(\"drive/My Drive/Cornell Birdcall Identification/input/valid_stage1_df.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIkR69N3rA4v"
      },
      "source": [
        "stage1_df = pd.read_csv(\"stage1_df.csv\")\n",
        "stage1_df[\"filename\"] = stage1_df[\"file_name\"].apply(lambda x: x.split('/')[-1].strip('.wav'))\n",
        "\n",
        "stage1_sudo_df = pd.read_csv(\"stage1_sudo.csv\")\n",
        "stage1_sudo_df[\"filename\"] = stage1_sudo_df[\"file_name\"].apply(lambda x: x.split('/')[-1].strip('.wav'))\n",
        "\n",
        "stage1_df = stage1_df.append(stage1_sudo_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOUiDZlqd7Fr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5723895-d3eb-49db-8275-f9f45cea2667"
      },
      "source": [
        "stage1_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(508712, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeeuIqks09Hj"
      },
      "source": [
        "stage1_df = stage1_df.merge(train_wav_df, on=\"filename\", how=\"left\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hKTpKiO1b9S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4a7a13fe-5cad-45ec-eeea-bbea2ab559a3"
      },
      "source": [
        "stage1_df.kfold.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    106505\n",
              "2    102991\n",
              "1    102384\n",
              "3    101895\n",
              "4     94937\n",
              "Name: kfold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2Zt8DijfKwf"
      },
      "source": [
        "stage1_df[\"nkfold\"] = -1\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold_id, (train_idx, valid_idx) in enumerate(skf.split(stage1_df, stage1_df[\"pred_code\"])):\n",
        "    stage1_df.iloc[valid_idx, -1] = fold_id\n",
        "\n",
        "#train_wav_df.to_csv(\"drive/My Drive/Cornell Birdcall Identification/input/train_spec_df.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCUikg6Efd6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "16f35ea3-586c-450f-f2b8-9e95532eee68"
      },
      "source": [
        "stage1_df.nkfold.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    101743\n",
              "0    101743\n",
              "4    101742\n",
              "3    101742\n",
              "2    101742\n",
              "Name: nkfold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUSPwTxu-c8X"
      },
      "source": [
        "stage1_df.to_csv(\"drive/My Drive/Cornell Birdcall Identification/input/stage1_fold_df.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mabcPZosoHqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dedf97c-cd28-4743-f4e8-cac5bfc6a289"
      },
      "source": [
        "%%writefile pytorch_utils.py\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def move_data_to_device(x, device):\n",
        "    if 'float' in str(x.dtype):\n",
        "        x = torch.Tensor(x)\n",
        "    elif 'int' in str(x.dtype):\n",
        "        x = torch.LongTensor(x)\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "    return x.to(device)\n",
        "\n",
        "\n",
        "def do_mixup(x, mixup_lambda):\n",
        "    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n",
        "    (1, 3, 5, ...).\n",
        "    Args:\n",
        "      x: (batch_size * 2, ...)\n",
        "      mixup_lambda: (batch_size * 2,)\n",
        "    Returns:\n",
        "      out: (batch_size, ...)\n",
        "    \"\"\"\n",
        "    \n",
        "    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n",
        "        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n",
        "        \n",
        "    return out\n",
        "\n",
        "class Mixup(object):\n",
        "    def __init__(self, mixup_alpha, random_seed=1234):\n",
        "        \"\"\"Mixup coefficient generator.\n",
        "        \"\"\"\n",
        "        self.mixup_alpha = mixup_alpha\n",
        "        self.random_state = np.random.RandomState(random_seed)\n",
        "\n",
        "    def get_lambda(self, batch_size):\n",
        "        \"\"\"Get mixup random coefficients.\n",
        "        Args:\n",
        "          batch_size: int\n",
        "        Returns:\n",
        "          mixup_lambdas: (batch_size,)\n",
        "        \"\"\"\n",
        "        mixup_lambdas = []\n",
        "        for n in range(0, batch_size, 2):\n",
        "            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n",
        "            mixup_lambdas.append(lam)\n",
        "            mixup_lambdas.append(1. - lam)\n",
        "\n",
        "        return np.array(mixup_lambdas)\n",
        "    \n",
        "\n",
        "def append_to_dict(dict, key, value):\n",
        "    if key in dict.keys():\n",
        "        dict[key].append(value)\n",
        "    else:\n",
        "        dict[key] = [value]\n",
        "\n",
        "\n",
        "def forward(model, generator, return_input=False, \n",
        "    return_target=False):\n",
        "    \"\"\"Forward data to a model.\n",
        "    \n",
        "    Args: \n",
        "      model: object\n",
        "      generator: object\n",
        "      return_input: bool\n",
        "      return_target: bool\n",
        "    Returns:\n",
        "      audio_name: (audios_num,)\n",
        "      clipwise_output: (audios_num, classes_num)\n",
        "      (ifexist) segmentwise_output: (audios_num, segments_num, classes_num)\n",
        "      (ifexist) framewise_output: (audios_num, frames_num, classes_num)\n",
        "      (optional) return_input: (audios_num, segment_samples)\n",
        "      (optional) return_target: (audios_num, classes_num)\n",
        "    \"\"\"\n",
        "    output_dict = {}\n",
        "    device = next(model.parameters()).device\n",
        "    time1 = time.time()\n",
        "\n",
        "    # Forward data to a model in mini-batches\n",
        "    for n, batch_data_dict in enumerate(generator):\n",
        "        print(n)\n",
        "        batch_waveform = move_data_to_device(batch_data_dict['waveform'], device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            batch_output = model(batch_waveform)\n",
        "\n",
        "        append_to_dict(output_dict, 'audio_name', batch_data_dict['audio_name'])\n",
        "\n",
        "        append_to_dict(output_dict, 'clipwise_output', \n",
        "            batch_output['clipwise_output'].data.cpu().numpy())\n",
        "\n",
        "        if 'segmentwise_output' in batch_output.keys():\n",
        "            append_to_dict(output_dict, 'segmentwise_output', \n",
        "                batch_output['segmentwise_output'].data.cpu().numpy())\n",
        "\n",
        "        if 'framewise_output' in batch_output.keys():\n",
        "            append_to_dict(output_dict, 'framewise_output', \n",
        "                batch_output['framewise_output'].data.cpu().numpy())\n",
        "            \n",
        "        if return_input:\n",
        "            append_to_dict(output_dict, 'waveform', batch_data_dict['waveform'])\n",
        "            \n",
        "        if return_target:\n",
        "            if 'target' in batch_data_dict.keys():\n",
        "                append_to_dict(output_dict, 'target', batch_data_dict['target'])\n",
        "\n",
        "        if n % 10 == 0:\n",
        "            print(' --- Inference time: {:.3f} s / 10 iterations ---'.format(\n",
        "                time.time() - time1))\n",
        "            time1 = time.time()\n",
        "\n",
        "    for key in output_dict.keys():\n",
        "        output_dict[key] = np.concatenate(output_dict[key], axis=0)\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def interpolate(x, ratio):\n",
        "    \"\"\"Interpolate data in time domain. This is used to compensate the \n",
        "    resolution reduction in downsampling of a CNN.\n",
        "    \n",
        "    Args:\n",
        "      x: (batch_size, time_steps, classes_num)\n",
        "      ratio: int, ratio to interpolate\n",
        "    Returns:\n",
        "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
        "    \"\"\"\n",
        "    (batch_size, time_steps, classes_num) = x.shape\n",
        "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
        "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
        "    return upsampled\n",
        "\n",
        "\n",
        "def pad_framewise_output(framewise_output, frames_num):\n",
        "    \"\"\"Pad framewise_output to the same length as input frames. The pad value \n",
        "    is the same as the value of the last frame.\n",
        "    Args:\n",
        "      framewise_output: (batch_size, frames_num, classes_num)\n",
        "      frames_num: int, number of frames to pad\n",
        "    Outputs:\n",
        "      output: (batch_size, frames_num, classes_num)\n",
        "    \"\"\"\n",
        "    pad = framewise_output[:, -1 :, :].repeat(1, frames_num - framewise_output.shape[1], 1)\n",
        "    \"\"\"tensor for padding\"\"\"\n",
        "\n",
        "    output = torch.cat((framewise_output, pad), dim=1)\n",
        "    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def count_flops(model, audio_length):\n",
        "    \"\"\"Count flops. Code modified from others' implementation.\n",
        "    \"\"\"\n",
        "    multiply_adds = True\n",
        "    list_conv2d=[]\n",
        "    def conv2d_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        " \n",
        "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n",
        "        bias_ops = 1 if self.bias is not None else 0\n",
        " \n",
        "        params = output_channels * (kernel_ops + bias_ops)\n",
        "        flops = batch_size * params * output_height * output_width\n",
        " \n",
        "        list_conv2d.append(flops)\n",
        "\n",
        "    list_conv1d=[]\n",
        "    def conv1d_hook(self, input, output):\n",
        "        batch_size, input_channels, input_length = input[0].size()\n",
        "        output_channels, output_length = output[0].size()\n",
        " \n",
        "        kernel_ops = self.kernel_size[0] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n",
        "        bias_ops = 1 if self.bias is not None else 0\n",
        " \n",
        "        params = output_channels * (kernel_ops + bias_ops)\n",
        "        flops = batch_size * params * output_length\n",
        " \n",
        "        list_conv1d.append(flops)\n",
        " \n",
        "    list_linear=[] \n",
        "    def linear_hook(self, input, output):\n",
        "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
        " \n",
        "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
        "        bias_ops = self.bias.nelement()\n",
        " \n",
        "        flops = batch_size * (weight_ops + bias_ops)\n",
        "        list_linear.append(flops)\n",
        " \n",
        "    list_bn=[] \n",
        "    def bn_hook(self, input, output):\n",
        "        list_bn.append(input[0].nelement() * 2)\n",
        " \n",
        "    list_relu=[] \n",
        "    def relu_hook(self, input, output):\n",
        "        list_relu.append(input[0].nelement() * 2)\n",
        " \n",
        "    list_pooling2d=[]\n",
        "    def pooling2d_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        " \n",
        "        kernel_ops = self.kernel_size * self.kernel_size\n",
        "        bias_ops = 0\n",
        "        params = output_channels * (kernel_ops + bias_ops)\n",
        "        flops = batch_size * params * output_height * output_width\n",
        " \n",
        "        list_pooling2d.append(flops)\n",
        "\n",
        "    list_pooling1d=[]\n",
        "    def pooling1d_hook(self, input, output):\n",
        "        batch_size, input_channels, input_length = input[0].size()\n",
        "        output_channels, output_length = output[0].size()\n",
        " \n",
        "        kernel_ops = self.kernel_size[0]\n",
        "        bias_ops = 0\n",
        "        \n",
        "        params = output_channels * (kernel_ops + bias_ops)\n",
        "        flops = batch_size * params * output_length\n",
        " \n",
        "        list_pooling2d.append(flops)\n",
        " \n",
        "    def foo(net):\n",
        "        childrens = list(net.children())\n",
        "        if not childrens:\n",
        "            if isinstance(net, nn.Conv2d):\n",
        "                net.register_forward_hook(conv2d_hook)\n",
        "            elif isinstance(net, nn.Conv1d):\n",
        "                net.register_forward_hook(conv1d_hook)\n",
        "            elif isinstance(net, nn.Linear):\n",
        "                net.register_forward_hook(linear_hook)\n",
        "            elif isinstance(net, nn.BatchNorm2d) or isinstance(net, nn.BatchNorm1d):\n",
        "                net.register_forward_hook(bn_hook)\n",
        "            elif isinstance(net, nn.ReLU):\n",
        "                net.register_forward_hook(relu_hook)\n",
        "            elif isinstance(net, nn.AvgPool2d) or isinstance(net, nn.MaxPool2d):\n",
        "                net.register_forward_hook(pooling2d_hook)\n",
        "            elif isinstance(net, nn.AvgPool1d) or isinstance(net, nn.MaxPool1d):\n",
        "                net.register_forward_hook(pooling1d_hook)\n",
        "            else:\n",
        "                print('Warning: flop of module {} is not counted!'.format(net))\n",
        "            return\n",
        "        for c in childrens:\n",
        "            foo(c)\n",
        "\n",
        "    # Register hook\n",
        "    foo(model)\n",
        "    \n",
        "    device = device = next(model.parameters()).device\n",
        "    input = torch.rand(1, audio_length).to(device)\n",
        "\n",
        "    out = model(input)\n",
        " \n",
        "    total_flops = sum(list_conv2d) + sum(list_conv1d) + sum(list_linear) + \\\n",
        "        sum(list_bn) + sum(list_relu) + sum(list_pooling2d) + sum(list_pooling1d)\n",
        "    \n",
        "    return total_flops"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing pytorch_utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9bb7LDhfj0l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ac11a39-0899-4ac9-fc71-5220a6f2f0db"
      },
      "source": [
        "%%writefile classifiers.py\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from timm.models.efficientnet import tf_efficientnet_b4_ns, tf_efficientnet_b3_ns, \\\n",
        "    tf_efficientnet_b5_ns, tf_efficientnet_b2_ns, tf_efficientnet_b6_ns, tf_efficientnet_b7_ns, tf_efficientnet_b0_ns\n",
        "from torch import nn\n",
        "from torch.nn.modules.dropout import Dropout\n",
        "from torch.nn.modules.linear import Linear\n",
        "from torch.nn.modules.pooling import AdaptiveAvgPool2d, AdaptiveMaxPool2d\n",
        "\n",
        "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
        "from torchlibrosa.augmentation import SpecAugmentation\n",
        "\n",
        "from pytorch_utils import do_mixup, interpolate, pad_framewise_output\n",
        "\n",
        "encoder_params = {\n",
        "    \"resnest50d\" : {\n",
        "        \"features\" : 2048,\n",
        "        \"init_op\"  : partial(timm.models.resnest50d, pretrained=True, in_chans=1)\n",
        "    },\n",
        "    \"densenet201\" : {\n",
        "        \"features\": 1920,\n",
        "        \"init_op\": partial(timm.models.densenet201, pretrained=True)\n",
        "    },\n",
        "    \"dpn92\" : {\n",
        "        \"features\": 2688,\n",
        "        \"init_op\": partial(timm.models.dpn92, pretrained=True)\n",
        "    },\n",
        "    \"dpn131\": {\n",
        "        \"features\": 2688,\n",
        "        \"init_op\": partial(timm.models.dpn131, pretrained=True)\n",
        "    },\n",
        "    \"tf_efficientnet_b0_ns\": {\n",
        "        \"features\": 1280,\n",
        "        \"init_op\": partial(tf_efficientnet_b0_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b3_ns\": {\n",
        "        \"features\": 1536,\n",
        "        \"init_op\": partial(tf_efficientnet_b3_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    },\n",
        "    \"tf_efficientnet_b2_ns\": {\n",
        "        \"features\": 1408,\n",
        "        \"init_op\": partial(tf_efficientnet_b2_ns, pretrained=True, drop_path_rate=0.2)\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def init_layer(layer):\n",
        "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
        "    nn.init.xavier_uniform_(layer.weight)\n",
        " \n",
        "    if hasattr(layer, 'bias'):\n",
        "        if layer.bias is not None:\n",
        "            layer.bias.data.fill_(0.)\n",
        "            \n",
        "    \n",
        "def init_bn(bn):\n",
        "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
        "    bn.bias.data.fill_(0.)\n",
        "    bn.weight.data.fill_(1.)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \n",
        "        super(ConvBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=(3, 3), stride=(1, 1),\n",
        "                              padding=(1, 1), bias=False)\n",
        "                              \n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels, \n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=(3, 3), stride=(1, 1),\n",
        "                              padding=(1, 1), bias=False)\n",
        "                              \n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.init_weight()\n",
        "        \n",
        "    def init_weight(self):\n",
        "        init_layer(self.conv1)\n",
        "        init_layer(self.conv2)\n",
        "        init_bn(self.bn1)\n",
        "        init_bn(self.bn2)\n",
        "\n",
        "        \n",
        "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
        "        \n",
        "        x = input\n",
        "        x = F.relu_(self.bn1(self.conv1(x)))\n",
        "        x = F.relu_(self.bn2(self.conv2(x)))\n",
        "        if pool_type == 'max':\n",
        "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg':\n",
        "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg+max':\n",
        "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            raise Exception('Incorrect argument!')\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock5x5(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        \n",
        "        super(ConvBlock5x5, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, \n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=(5, 5), stride=(1, 1),\n",
        "                              padding=(2, 2), bias=False)\n",
        "                              \n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.init_weight()\n",
        "        \n",
        "    def init_weight(self):\n",
        "        init_layer(self.conv1)\n",
        "        init_bn(self.bn1)\n",
        "\n",
        "        \n",
        "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
        "        \n",
        "        x = input\n",
        "        x = F.relu_(self.bn1(self.conv1(x)))\n",
        "        if pool_type == 'max':\n",
        "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg':\n",
        "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg+max':\n",
        "            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "            x2 = F.max_pool2d(x, kernel_size=pool_size)\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            raise Exception('Incorrect argument!')\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class AttBlock(nn.Module):\n",
        "    def __init__(self, n_in, n_out, activation='linear', temperature=1.):\n",
        "        super(AttBlock, self).__init__()\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.temperature = temperature\n",
        "        self.att = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        self.cla = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "        \n",
        "        self.bn_att = nn.BatchNorm1d(n_out)\n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        init_layer(self.att)\n",
        "        init_layer(self.cla)\n",
        "        init_bn(self.bn_att)\n",
        "         \n",
        "    def forward(self, x):\n",
        "        # x: (n_samples, n_in, n_time)\n",
        "        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n",
        "        cla = self.nonlinear_transform(self.cla(x))\n",
        "        x = torch.sum(norm_att * cla, dim=2)\n",
        "        return x, norm_att, cla\n",
        "\n",
        "    def nonlinear_transform(self, x):\n",
        "        if self.activation == 'linear':\n",
        "            return x\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class Cnn14_16k(nn.Module):\n",
        "    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num):\n",
        "        \n",
        "        super(Cnn14_16k, self).__init__() \n",
        "\n",
        "        assert sample_rate == 16000\n",
        "        assert window_size == 512\n",
        "        assert hop_size == 160\n",
        "        assert mel_bins == 64\n",
        "        assert fmin == 50\n",
        "        assert fmax == 8000\n",
        "\n",
        "        window = 'hann'\n",
        "        center = True\n",
        "        pad_mode = 'reflect'\n",
        "        ref = 1.0\n",
        "        amin = 1e-10\n",
        "        top_db = None\n",
        "\n",
        "        # Spectrogram extractor\n",
        "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
        "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Logmel feature extractor\n",
        "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
        "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Spec augmenter\n",
        "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
        "            freq_drop_width=8, freq_stripes_num=2)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
        "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
        "        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n",
        "        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 2048, bias=True)\n",
        "        self.fc_audioset1 = nn.Linear(2048, classes_num, bias=True)\n",
        "        \n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        init_bn(self.bn0)\n",
        "        init_layer(self.fc1)\n",
        "        init_layer(self.fc_audioset1)\n",
        " \n",
        "    def forward(self, input, spec_aug=False, mixup_lambda=None):\n",
        "        \"\"\"\n",
        "        Input: (batch_size, data_length)\"\"\"\n",
        "\n",
        "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
        "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
        "        #x = input\n",
        "        x = x.transpose(1, 3)\n",
        "        x = self.bn0(x)\n",
        "        x = x.transpose(1, 3)\n",
        "\n",
        "        if spec_aug:\n",
        "            x = self.spec_augmenter(x)\n",
        "\n",
        "        # Mixup on spectrogram\n",
        "        if mixup_lambda is not None:\n",
        "            x = do_mixup(x, mixup_lambda)\n",
        "        \n",
        "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = torch.mean(x, dim=3)\n",
        "        \n",
        "        (x1, _) = torch.max(x, dim=2)\n",
        "        x2 = torch.mean(x, dim=2)\n",
        "        x = x1 + x2\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu_(self.fc1(x))\n",
        "        #embedding = F.dropout(x, p=0.5, training=self.training)\n",
        "        clipwise_output = self.fc_audioset1(x)\n",
        "        \n",
        "        #output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n",
        "\n",
        "        return clipwise_output\n",
        "\n",
        "class BirdClassifier(nn.Module):\n",
        "    def __init__(self, encoder, sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num):\n",
        "        super().__init__()\n",
        "\n",
        "        window = 'hann'\n",
        "        center = True\n",
        "        pad_mode = 'reflect'\n",
        "        ref = 1.0\n",
        "        amin = 1e-10\n",
        "        top_db = None\n",
        "        \n",
        "        # Spectrogram extractor\n",
        "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n",
        "            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Logmel feature extractor\n",
        "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n",
        "            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n",
        "            freeze_parameters=True)\n",
        "\n",
        "        # Spec augmenter\n",
        "        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n",
        "            freq_drop_width=8, freq_stripes_num=2)\n",
        "\n",
        "        self.encoder = encoder_params[encoder][\"init_op\"]()\n",
        "        self.avg_pool = AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.fc = Linear(encoder_params[encoder]['features'], classes_num)\n",
        "\n",
        "    def forward(self, input, spec_aug=False, mixup_lambda=None):\n",
        "\n",
        "        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
        "        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
        "\n",
        "        if spec_aug:\n",
        "            x = self.spec_augmenter(x)\n",
        "\n",
        "        # Mixup on spectrogram\n",
        "        if mixup_lambda is not None:\n",
        "            x = do_mixup(x, mixup_lambda)\n",
        "\n",
        "        x = self.encoder.forward_features(x)\n",
        "        x = self.avg_pool(x).flatten(1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing classifiers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jphJgoJPfjyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d5d772f-1bb4-4529-e024-f0101d212b82"
      },
      "source": [
        "%%writefile dataset.py\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "from albumentations.pytorch.functional import img_to_tensor\n",
        "\n",
        "from bird_codes import BIRD_CODE, INV_BIRD_CODE\n",
        "\n",
        "class PANNsDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_list: List[List[str]],\n",
        "        period=1,\n",
        "        transforms=None,\n",
        "        train=True,\n",
        "        waveform_transforms=None):\n",
        "\n",
        "        self.file_list = file_list\n",
        "        self.period = period\n",
        "        self.transforms = transforms\n",
        "        self.train = train\n",
        "        self.waveform_transforms = waveform_transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "    \n",
        "    def __getitem__(self, idx: int):\n",
        "        \n",
        "        wav_path, ebird_code = self.file_list[idx]\n",
        "\n",
        "        y, sr = sf.read(wav_path)\n",
        "\n",
        "        if not self.train:\n",
        "            self.period = 5\n",
        "\n",
        "        if self.waveform_transforms:\n",
        "            y = self.waveform_transforms(y)\n",
        "        else:\n",
        "            len_y = len(y)\n",
        "            effective_length = sr * self.period\n",
        "            if len_y < effective_length:\n",
        "                new_y = np.zeros(effective_length, dtype=y.dtype)\n",
        "                start = np.random.randint(effective_length - len_y)\n",
        "                new_y[start:start + len_y] = y\n",
        "                y = new_y.astype(np.float32)\n",
        "            elif len_y > effective_length:\n",
        "                start = np.random.randint(len_y - effective_length)\n",
        "                y = y[start:start + effective_length].astype(np.float32)\n",
        "            else:\n",
        "                y = y.astype(np.float32)\n",
        "        \n",
        "        #if self.transforms:\n",
        "        #    pass\n",
        "            #spec, sr = self.transforms(data=(y, sr))['data']\n",
        "            #spec = np.expand_dims(spec, axis=0) #np.transpose(spec, (2, 0, 1)).astype(np.float32)\n",
        "\n",
        "            \n",
        "        labels = np.zeros(len(BIRD_CODE), dtype=\"f\")\n",
        "        if self.train:\n",
        "            labels[BIRD_CODE[ebird_code]] = 1\n",
        "        #labels = BIRD_CODE[ebird_code]\n",
        "        else:\n",
        "            ebird_code = ebird_code.split(' ')\n",
        "            #print(ebird_code)\n",
        "            for eb in ebird_code:\n",
        "                labels[BIRD_CODE[eb]] = 1\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"waveform\" : y, #torch.tensor(spec, dtype=torch.float),\n",
        "            \"targets\" : labels, #torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __get_labels__(self):\n",
        "        return np.array(self.file_list)[:, 1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s3_PcYjlELb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a894f54-5963-4742-a690-814b2a59610c"
      },
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "def convert_onehot(x, max=54):\n",
        "    return np.eye(max)[x]\n",
        "\n",
        "def row_wise_f1_score_micro_numpy(y_true, y_pred, threshold=0.5, count=5):\n",
        "    \"\"\" \n",
        "    @author shonenkov \n",
        "    \n",
        "    y_true - 2d npy vector with gt\n",
        "    y_pred - 2d npy vector with prediction\n",
        "    threshold - for round labels\n",
        "    count - number of preds (used sorting by confidence)\n",
        "    \"\"\"\n",
        "    def meth_agn_v2(x, threshold):\n",
        "        idx, = np.where(x > threshold)\n",
        "        return idx[np.argsort(x[idx])[::-1]]\n",
        "\n",
        "    F1 = []\n",
        "    for preds, trues in zip(y_pred, y_true):\n",
        "        TP, FN, FP = 0, 0, 0\n",
        "        preds = meth_agn_v2(preds, threshold)[:count]\n",
        "        trues = meth_agn_v2(trues, threshold)\n",
        "        for true in trues:\n",
        "            if true in preds:\n",
        "                TP += 1\n",
        "            else:\n",
        "                FN += 1\n",
        "        for pred in preds:\n",
        "            if pred not in trues:\n",
        "                FP += 1\n",
        "        F1.append(2*TP / (2*TP + FN + FP))\n",
        "    return np.mean(F1)\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class MetricMeter(object):\n",
        "\n",
        "    def __init__(self, train=True):\n",
        "        self.reset()\n",
        "        self.train = train\n",
        "    \n",
        "    def reset(self):\n",
        "        self.y_true = []\n",
        "        self.y_pred = []\n",
        "        self.y_true_ = []\n",
        "        self.y_pred_ = []\n",
        "        self.acc = 0\n",
        "        self.f1 = 0\n",
        "        self.map = 0\n",
        "    \n",
        "    def update(self, y_true, y_pred):\n",
        "        self.y_true.extend(y_true.cpu().detach().numpy().tolist()) \n",
        "        self.y_pred.extend(torch.sigmoid(y_pred).cpu().detach().numpy().tolist())\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def avg(self):\n",
        "\n",
        "        #self.y_true = np.concatenate(self.y_true, axis=0)\n",
        "        #self.y_pred = np.concatenate(self.y_pred, axis=0)\n",
        "\n",
        "        #print(self.y_true[0])\n",
        "        #print(self.y_pred[0])\n",
        "\n",
        "        if self.train:\n",
        "            self.acc = metrics.accuracy_score(np.argmax(self.y_true, axis=1), np.argmax(self.y_pred, axis=1))\n",
        "            self.auc = 0#metrics.roc_auc_score(self.y_true, self.y_pred, average=None)\n",
        "            self.f1 = 0#metrics.f1_score(np.argmax(self.y_true, axis=1), np.argmax(self.y_true, axis=1), average=None)\n",
        "            self.map = 0#np.mean(metrics.average_precision_score(self.y_true, self.y_pred)\n",
        "            self.row_f1 = 0\n",
        "        else:\n",
        "            self.acc = metrics.accuracy_score(np.argmax(self.y_true, axis=1), np.argmax(self.y_pred, axis=1))\n",
        "            self.auc = 0 #np.mean(metrics.roc_auc_score(self.y_true, self.y_pred, average=None))\n",
        "            self.f1 = metrics.f1_score(np.argmax(self.y_true, axis=1), np.argmax(self.y_pred, axis=1), average='macro')\n",
        "            self.map = np.nan_to_num(\n",
        "                    np.mean(metrics.average_precision_score(self.y_true, self.y_pred, average=None))\n",
        "                ).mean()\n",
        "            \n",
        "            self.row_f1 = row_wise_f1_score_micro_numpy(np.array(self.y_true), np.array(self.y_pred))\n",
        "\n",
        "        return {\n",
        "            \"acc\" : self.acc,\n",
        "            \"f1\" : self.f1,\n",
        "            \"auc\" : self.auc,\n",
        "            \"map\" : self.map,\n",
        "            \"row_f1\" : self.row_f1\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke8yaXuPlEBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d63ec035-ead0-4335-b93c-cefbaefb8106"
      },
      "source": [
        "%%writefile losses.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PANNsLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.cel = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        input_ = input\n",
        "        input_ = torch.where(\n",
        "            torch.isnan(input_),\n",
        "            torch.zeros_like(input_),\n",
        "            input_\n",
        "        )\n",
        "        input_ = torch.where(\n",
        "            torch.isinf(input_),\n",
        "            torch.zeros_like(input_),\n",
        "            input_\n",
        "        )\n",
        "\n",
        "        target = target.float()\n",
        "        \"\"\"\n",
        "\n",
        "        #return self.bce(input_, target)\n",
        "        return self.bce(input, target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing losses.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6o8mDWalD0j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10c9b65a-0840-4b7c-ec0a-60def2b4efc5"
      },
      "source": [
        "%%writefile bird_codes.py\n",
        "\n",
        "\"\"\"\n",
        "BIRD_CODE = {\n",
        "    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n",
        "    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n",
        "    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n",
        "    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n",
        "    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n",
        "    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n",
        "    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n",
        "    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n",
        "    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n",
        "    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n",
        "    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'nocall': 54\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BIRD_CODE = {\n",
        "    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n",
        "    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n",
        "    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n",
        "    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n",
        "    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n",
        "    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n",
        "    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n",
        "    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n",
        "    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n",
        "    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n",
        "    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n",
        "    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n",
        "    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n",
        "    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n",
        "    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n",
        "    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n",
        "    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n",
        "    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n",
        "    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n",
        "    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n",
        "    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n",
        "    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n",
        "    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n",
        "    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n",
        "    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n",
        "    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n",
        "    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n",
        "    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n",
        "    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n",
        "    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n",
        "    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n",
        "    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n",
        "    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n",
        "    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n",
        "    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n",
        "    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n",
        "    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n",
        "    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n",
        "    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n",
        "    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n",
        "    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n",
        "    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n",
        "    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n",
        "    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n",
        "    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n",
        "    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n",
        "    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n",
        "    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n",
        "    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n",
        "    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n",
        "    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n",
        "    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n",
        "    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n",
        "}\n",
        "\n",
        "INV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing bird_codes.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-mgwp87oyBj"
      },
      "source": [
        "!pip -q install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git >> quit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMOzETGJo0Ik",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9faf27b3-fc49-443b-e76f-f5754cec865c"
      },
      "source": [
        "%%writefile schedulers.py\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "# Fix Warmup Bug\n",
        "from warmup_scheduler import GradualWarmupScheduler  # https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
        "\n",
        "\n",
        "class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n",
        "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
        "        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "        if self.multiplier == 1.0:\n",
        "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing schedulers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP8m1KOOo1rl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf9bd0e-740b-49d3-e7c0-6e6071bf0954"
      },
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import cv2\n",
        "import audioread\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import librosa\n",
        "import librosa.display as display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "from contextlib import contextmanager\n",
        "from IPython.display import Audio\n",
        "from pathlib import Path\n",
        "from typing import Optional, List\n",
        "\n",
        "from catalyst.data.sampler import DistributedSampler, BalanceClassSampler\n",
        "#from catalyst.dl import SupervisedRunner, State, CallbackOrder, Callback, CheckpointCallback\n",
        "from fastprogress import progress_bar\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, average_precision_score\n",
        "\n",
        "import classifiers\n",
        "import losses\n",
        "import dataset\n",
        "from config import args\n",
        "from utils import AverageMeter, MetricMeter\n",
        "from schedulers import GradualWarmupSchedulerV2\n",
        "from pytorch_utils import do_mixup, Mixup, move_data_to_device\n",
        "\n",
        "mixup_augmenter = Mixup(mixup_alpha=1.)\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    \n",
        "    #if alpha > 0:\n",
        "    #    lam = np.random.beta(alpha, alpha)\n",
        "    #else:\n",
        "    #    lam = 1\n",
        "    \n",
        "    batch_size = x.size()[0]\n",
        "\n",
        "    if use_cuda:\n",
        "        index0 = torch.randperm(batch_size).cuda()\n",
        "        index1 = torch.randperm(batch_size).cuda()\n",
        "        index2 = torch.randperm(batch_size).cuda()\n",
        "        index3 = torch.randperm(batch_size).cuda()\n",
        "        index4 = torch.randperm(batch_size).cuda()\n",
        "\n",
        "    else:\n",
        "        index = torch.randperam(bath_size)\n",
        "    \n",
        "    #mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "\n",
        "    ind = random.choice([0,1,2,3,4])\n",
        "\n",
        "    if ind == 0:\n",
        "        mixed_x = x\n",
        "        mixed_y = y\n",
        "    elif ind == 1:\n",
        "        mixed_x = torch.cat([x, x[index1, :]], dim=1)\n",
        "        mixed_y = y + y[index1, :]\n",
        "    elif ind == 2:\n",
        "        mixed_x = torch.cat([x, x[index1, :], x[index2]], dim=1)\n",
        "        mixed_y = y + y[index1, :] + y[index2, :]\n",
        "    elif ind == 3:\n",
        "        mixed_x = torch.cat([x, x[index1, :], x[index2], x[index3, :]], dim=1)\n",
        "        mixed_y = y + y[index1, :] + y[index2, :] + y[index3, :]\n",
        "    elif ind == 4:\n",
        "        mixed_x = torch.cat([x, x[index1, :], x[index2], x[index3, :], x[index4, :]], dim=1)\n",
        "        mixed_y = y + y[index1, :] + y[index2, :] + y[index3, :] + y[index4, :]\n",
        "    \n",
        "    mixed_y = torch.clamp(mixed_y, min=0, max=1)\n",
        "\n",
        "    \"\"\"\n",
        "    if np.random.random() > 0.5:\n",
        "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "        mixed_y = lam * y + (1 - lam) * y[index, :]\n",
        "    else:\n",
        "        mixed_x =  x + x[index, :]\n",
        "        mixed_y =  y + y[index, :]\n",
        "        \n",
        "    \"\"\"\n",
        "    return mixed_x, mixed_y\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def train_epoch(args, model, loader, criterion, optimizer, epoch):\n",
        "    losses = AverageMeter()\n",
        "    scores = MetricMeter(train=True)\n",
        "\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    t = tqdm(loader, total=500)\n",
        "    for i, sample in enumerate(t):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input = sample['waveform'].to(args.device)\n",
        "        target = sample['targets'].to(args.device)\n",
        "\n",
        "        if args.new_mixup:\n",
        "            input, target = mixup_data(input, target, args.alpha, True)\n",
        "\n",
        "        if args.mixup:\n",
        "            mixup_lambda = mixup_augmenter.get_lambda(\n",
        "                batch_size=len(input)\n",
        "            )\n",
        "            mixup_lambda = move_data_to_device(mixup_lambda, args.device)\n",
        "\n",
        "            target = do_mixup(target, mixup_lambda)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            if args.mixup:\n",
        "                output = model(input, True, mixup_lambda)\n",
        "            else:\n",
        "                output = model(input, True)\n",
        "            \n",
        "            #print(target.sum(dim=1))\n",
        "            #print(input.shape)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "        #scheduler.step()\n",
        "\n",
        "        bs = input.size(0)\n",
        "        scores.update(target, output)\n",
        "        losses.update(loss.item(), bs)\n",
        "\n",
        "        t.set_description(f\"Train E:{epoch} - Loss:{losses.avg:0.4f}\")\n",
        "\n",
        "        if i == 500:\n",
        "            break\n",
        "\n",
        "    t.close()\n",
        "\n",
        "    return scores.avg, losses.avg\n",
        "\n",
        "def valid_epoch(args, model, loader, criterion, epoch):\n",
        "    losses = AverageMeter()\n",
        "    scores = MetricMeter(train=False)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        t = tqdm(loader)\n",
        "        for i, sample in enumerate(t):\n",
        "\n",
        "            input = sample['waveform'].to(args.device)\n",
        "            target = sample['targets'].to(args.device)\n",
        "\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            bs = input.size(0)\n",
        "            scores.update(target, output)\n",
        "            losses.update(loss.item(), bs)\n",
        "\n",
        "            t.set_description(f\"Valid E:{epoch} - Loss:{losses.avg:0.4f}\")\n",
        "\n",
        "    t.close()\n",
        "\n",
        "    return scores.avg, losses.avg\n",
        "\n",
        "def main(fold):\n",
        "\n",
        "    # Setting seed\n",
        "    seed = args.seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    args.fold = fold\n",
        "    args.save_path = os.path.join(args.output_dir, args.exp_name)\n",
        "    os.makedirs(args.save_path, exist_ok=True)\n",
        "\n",
        "    model = classifiers.__dict__[args.network](**args.model_config)\n",
        "    if args.pretrained_path:\n",
        "        weights = torch.load(args.pretrained_path, map_location=args.device)\n",
        "        model.load_state_dict(weights[\"model\"], strict=False)\n",
        "    \n",
        "    model = model.to(args.device)\n",
        "\n",
        "    criterion = losses.__dict__[args.losses]()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.init_lr)\n",
        "\n",
        "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, args.epochs)\n",
        "    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n",
        "    \n",
        "    train_df = pd.read_csv(args.train_csv)\n",
        "    valid_df = pd.read_csv(args.valid_csv)\n",
        "    if args.DEBUG:\n",
        "        train_df = train_df.sample(1000)\n",
        "        valid_df = valid_df.sample(1000)\n",
        "    \n",
        "    train_fold = train_df#train_df[train_df.nkfold != fold]\n",
        "    valid_fold = valid_df#[train_df.nkfold.isin([fold, -1])]\n",
        "\n",
        "    train_file_list = train_fold[['path', 'pred_code']].values.tolist()\n",
        "    valid_file_list = valid_fold[['path', 'pred_code']].values.tolist()\n",
        "\n",
        "    train_dataset = dataset.PANNsDataset(\n",
        "        train_file_list,\n",
        "        period=args.PERIOD,\n",
        "        transforms=False,#train_transforms(args.mel_param),\n",
        "        train=True\n",
        "    )\n",
        "    valid_dataset = dataset.PANNsDataset(\n",
        "        valid_file_list,\n",
        "        period=args.PERIOD,\n",
        "        transforms=False,#valid_transforms(args.mel_param)\n",
        "        train=False\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        sampler = BalanceClassSampler(labels=train_dataset.__get_labels__(), mode=\"upsampling\"),\n",
        "        #shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=args.num_workers\n",
        "    )\n",
        "\n",
        "    print(\".................... Training started .................\")\n",
        "\n",
        "    best_map = -999999\n",
        "    best_f1  = -999999\n",
        "    best_row_f1 = -99999\n",
        "\n",
        "    if args.load_from:\n",
        "        weights = torch.load(os.path.join(\"drive/My Drive/Cornell Birdcall Identification/weights/Cnn14_16k_5\", f\"fold-{args.fold}.bin\"))\n",
        "        model.load_state_dict(weights[\"model\"])\n",
        "        #optimizer.load_state_dict(weights[\"optimizer\"])\n",
        "        #scheduler_warmup.load_state_dict(weights[\"scheduler_warmup\"])\n",
        "        #args.start_epoch = 0 #weights[\"epoch\"] + 1\n",
        "        #best_map = 0.6212948058165978\n",
        "        \n",
        "        model = model.to(args.device)\n",
        "\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        #scheduler_warmup.step(epoch)\n",
        "\n",
        "        train_avg, train_loss = train_epoch(\n",
        "            args,\n",
        "            model,\n",
        "            train_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            epoch\n",
        "        )\n",
        "\n",
        "        valid_avg, valid_loss = valid_epoch(\n",
        "            args,\n",
        "            model,\n",
        "            valid_loader,\n",
        "            criterion,\n",
        "            epoch\n",
        "        )\n",
        "\n",
        "        scheduler_warmup.step()    \n",
        "        if epoch==2: scheduler_warmup.step() # bug workaround \n",
        "\n",
        "        content = f\"\"\"\n",
        "                {time.ctime()} \\n\n",
        "                Fold:{args.fold}, Epoch:{epoch}, lr:{optimizer.param_groups[0]['lr']:.7}, \\n\n",
        "                Train Loss:{train_loss:0.4f} - ACC:{train_avg['acc']:0.4f} - F1:{train_avg['f1']:0.4f} - MAP:{train_avg['map']:0.4f}  \\n\n",
        "                Valid Loss:{valid_loss:0.4f} - ACC:{valid_avg['acc']:0.4f} - F1:{valid_avg['f1']:0.4f} - ROW_F1:{valid_avg['row_f1']:0.4f} - MAP:{valid_avg['map']:0.4f} \\n\\n\n",
        "        \"\"\"\n",
        "        print(content)\n",
        "\n",
        "        with open(f'{args.save_path}/log_{args.exp_name}.txt', 'a') as appender:\n",
        "            appender.write(content + '\\n')\n",
        "        \n",
        "        if valid_avg[\"row_f1\"] > best_row_f1:\n",
        "            print(f\"######### >>>>>>> Model Improved from {best_row_f1} -----> {valid_avg['row_f1']}\")\n",
        "            checkpoint = { \n",
        "                'epoch': epoch,\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'scheduler_warmup': scheduler_warmup.state_dict()\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, os.path.join(args.save_path, f\"checkpoint-fold-{args.fold}.bin\"))\n",
        "            torch.save(model.state_dict(), os.path.join(args.save_path, f\"fold-{args.fold}.bin\"))\n",
        "\n",
        "            best_row_f1 = valid_avg[\"row_f1\"]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    for fold in range(5):\n",
        "        print(\"#\"*20)\n",
        "        print(f\"####### FOLD : {fold}\")\n",
        "        if fold == 0:\n",
        "            main(fold)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_eB_GAZjwwo"
      },
      "source": [
        "#!wget https://zenodo.org/record/3987831/files/Cnn14_16k_mAP%3D0.438.pth?download=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9-rPo26o7UN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15b574fb-eb96-4f6d-c817-544c319e107f"
      },
      "source": [
        "%%writefile config.py\n",
        "\n",
        "class args:\n",
        "\n",
        "    DEBUG = False\n",
        "\n",
        "    mixup = False\n",
        "\n",
        "    new_mixup = True\n",
        "\n",
        "    alpha = 1\n",
        "\n",
        "    exp_name = \"stage1_Cnn14_16k_valid\"\n",
        "    output_dir = \"drive/My Drive/Cornell Birdcall Identification/weights\"\n",
        "    train_csv = \"drive/My Drive/Cornell Birdcall Identification/input/train_stage1_df.csv\"\n",
        "    valid_csv = \"drive/My Drive/Cornell Birdcall Identification/input/valid_stage1_df.csv\"\n",
        "    pretrained_path = False #\"Cnn14_16k_mAP=0.438.pth?download=1\"\n",
        "\n",
        "    network = \"Cnn14_16k\" #\"Cnn14_16k\" #\"BirdClassifier\"\n",
        "    encoder = \"resnest50d\"\n",
        "\n",
        "    losses = \"PANNsLoss\"\n",
        "\n",
        "    mel_param = {\n",
        "        #\"hop_lenght\" : 345 * 2,\n",
        "        \"fmin\" : 20,\n",
        "        \"fmax\" : 16000 // 2,\n",
        "        \"n_mels\" : 128,\n",
        "        \"n_fft\" : 128 * 20\n",
        "    }\n",
        "\n",
        "    model_config = {\n",
        "        #\"encoder\" : \"resnest50d\",\n",
        "        \"sample_rate\": 16000,\n",
        "        \"window_size\": 512,\n",
        "        \"hop_size\": 160,\n",
        "        \"mel_bins\": 64,\n",
        "        \"fmin\": 50,\n",
        "        \"fmax\": 8000,\n",
        "        \"classes_num\": 264 #209 #54 #264\n",
        "    }\n",
        "\n",
        "    PERIOD = 1\n",
        "    \n",
        "    device = \"cuda\"\n",
        "    seed = 1001\n",
        "    epochs = 150\n",
        "    batch_size = 32 * 4\n",
        "    num_workers = 4\n",
        "    start_epoch = 0\n",
        "\n",
        "    warmup_epo = 2\n",
        "    cosine_epo = 8\n",
        "    init_lr = 3e-5\n",
        "\n",
        "    load_from = True\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting config.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43a-8nL3C4oX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0297db00-95c3-4c51-e847-aea6f94d4b8d"
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####################\n",
            "####### FOLD : 0\n",
            ".................... Training started .................\n",
            "Train E:0 - Loss:0.0414: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:0 - Loss:0.1089: 100% 120/120 [01:09<00:00,  1.71it/s]\n",
            "\n",
            "                Thu Sep 10 05:32:06 2020 \n",
            "\n",
            "                Fold:0, Epoch:0, lr:0.0003, \n",
            "\n",
            "                Train Loss:0.0414 - ACC:0.2614 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1089 - ACC:0.0594 - F1:0.0306 - ROW_F1:0.1284 - MAP:0.0766 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from -99999 -----> 0.12842387659289067\n",
            "Train E:1 - Loss:0.0335: 100% 500/500 [05:07<00:00,  1.63it/s]\n",
            "Valid E:1 - Loss:0.1071: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 05:37:55 2020 \n",
            "\n",
            "                Fold:0, Epoch:1, lr:0.0003, \n",
            "\n",
            "                Train Loss:0.0335 - ACC:0.3056 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1071 - ACC:0.0616 - F1:0.0317 - ROW_F1:0.1695 - MAP:0.0884 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.12842387659289067 -----> 0.16951361918279703\n",
            "Train E:2 - Loss:0.0285: 100% 500/500 [05:00<00:00,  1.66it/s]\n",
            "Valid E:2 - Loss:0.1120: 100% 120/120 [00:31<00:00,  3.85it/s]\n",
            "\n",
            "                Thu Sep 10 05:43:38 2020 \n",
            "\n",
            "                Fold:0, Epoch:2, lr:0.0002998684, \n",
            "\n",
            "                Train Loss:0.0285 - ACC:0.3400 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1120 - ACC:0.0633 - F1:0.0337 - ROW_F1:0.1905 - MAP:0.0931 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.16951361918279703 -----> 0.1904756185692969\n",
            "Train E:3 - Loss:0.0268: 100% 500/500 [05:08<00:00,  1.62it/s]\n",
            "Valid E:3 - Loss:0.1153: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 05:49:29 2020 \n",
            "\n",
            "                Fold:0, Epoch:3, lr:0.000299704, \n",
            "\n",
            "                Train Loss:0.0268 - ACC:0.3457 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1153 - ACC:0.0677 - F1:0.0357 - ROW_F1:0.2000 - MAP:0.0944 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.1904756185692969 -----> 0.20001824902905804\n",
            "Train E:4 - Loss:0.0244: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:4 - Loss:0.1231: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 05:55:17 2020 \n",
            "\n",
            "                Fold:0, Epoch:4, lr:0.0002994739, \n",
            "\n",
            "                Train Loss:0.0244 - ACC:0.3596 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1231 - ACC:0.0637 - F1:0.0334 - ROW_F1:0.2182 - MAP:0.0985 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.20001824902905804 -----> 0.2181714576866886\n",
            "Train E:5 - Loss:0.0223: 100% 500/500 [05:03<00:00,  1.65it/s]\n",
            "Valid E:5 - Loss:0.1249: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 06:01:03 2020 \n",
            "\n",
            "                Fold:0, Epoch:5, lr:0.0002991783, \n",
            "\n",
            "                Train Loss:0.0223 - ACC:0.3684 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1249 - ACC:0.0633 - F1:0.0344 - ROW_F1:0.2245 - MAP:0.1005 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2181714576866886 -----> 0.2244783429257717\n",
            "Train E:6 - Loss:0.0208: 100% 500/500 [05:03<00:00,  1.65it/s]\n",
            "Valid E:6 - Loss:0.1284: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 06:06:49 2020 \n",
            "\n",
            "                Fold:0, Epoch:6, lr:0.0002988172, \n",
            "\n",
            "                Train Loss:0.0208 - ACC:0.3762 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1284 - ACC:0.0672 - F1:0.0361 - ROW_F1:0.2278 - MAP:0.1016 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2244783429257717 -----> 0.22775406963746303\n",
            "Train E:7 - Loss:0.0194: 100% 500/500 [05:02<00:00,  1.65it/s]\n",
            "Valid E:7 - Loss:0.1334: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 06:12:34 2020 \n",
            "\n",
            "                Fold:0, Epoch:7, lr:0.0002983908, \n",
            "\n",
            "                Train Loss:0.0194 - ACC:0.3843 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1334 - ACC:0.0654 - F1:0.0344 - ROW_F1:0.2377 - MAP:0.1030 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.22775406963746303 -----> 0.2377203791222789\n",
            "Train E:8 - Loss:0.0186: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:8 - Loss:0.1379: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 06:18:20 2020 \n",
            "\n",
            "                Fold:0, Epoch:8, lr:0.0002978994, \n",
            "\n",
            "                Train Loss:0.0186 - ACC:0.3777 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1379 - ACC:0.0624 - F1:0.0346 - ROW_F1:0.2434 - MAP:0.1041 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2377203791222789 -----> 0.2433611488049746\n",
            "Train E:9 - Loss:0.0171: 100% 500/500 [05:00<00:00,  1.66it/s]\n",
            "Valid E:9 - Loss:0.1411: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 06:24:04 2020 \n",
            "\n",
            "                Fold:0, Epoch:9, lr:0.0002973431, \n",
            "\n",
            "                Train Loss:0.0171 - ACC:0.3980 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1411 - ACC:0.0645 - F1:0.0353 - ROW_F1:0.2453 - MAP:0.1042 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2433611488049746 -----> 0.24529024274848057\n",
            "Train E:10 - Loss:0.0166: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:10 - Loss:0.1456: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 06:29:51 2020 \n",
            "\n",
            "                Fold:0, Epoch:10, lr:0.0002967221, \n",
            "\n",
            "                Train Loss:0.0166 - ACC:0.3829 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1456 - ACC:0.0680 - F1:0.0358 - ROW_F1:0.2501 - MAP:0.1049 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.24529024274848057 -----> 0.250136529772953\n",
            "Train E:11 - Loss:0.0160: 100% 500/500 [05:03<00:00,  1.65it/s]\n",
            "Valid E:11 - Loss:0.1472: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 06:35:36 2020 \n",
            "\n",
            "                Fold:0, Epoch:11, lr:0.0002960368, \n",
            "\n",
            "                Train Loss:0.0160 - ACC:0.3897 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1472 - ACC:0.0609 - F1:0.0348 - ROW_F1:0.2491 - MAP:0.1053 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:12 - Loss:0.0154: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:12 - Loss:0.1508: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 06:41:17 2020 \n",
            "\n",
            "                Fold:0, Epoch:12, lr:0.0002952875, \n",
            "\n",
            "                Train Loss:0.0154 - ACC:0.3940 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1508 - ACC:0.0646 - F1:0.0355 - ROW_F1:0.2555 - MAP:0.1058 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.250136529772953 -----> 0.2555089711394985\n",
            "Train E:13 - Loss:0.0147: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:13 - Loss:0.1558: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 06:47:06 2020 \n",
            "\n",
            "                Fold:0, Epoch:13, lr:0.0002944744, \n",
            "\n",
            "                Train Loss:0.0147 - ACC:0.3917 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1558 - ACC:0.0645 - F1:0.0355 - ROW_F1:0.2564 - MAP:0.1061 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2555089711394985 -----> 0.25641965488018553\n",
            "Train E:14 - Loss:0.0140: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:14 - Loss:0.1606: 100% 120/120 [00:31<00:00,  3.84it/s]\n",
            "\n",
            "                Thu Sep 10 06:52:56 2020 \n",
            "\n",
            "                Fold:0, Epoch:14, lr:0.0002935979, \n",
            "\n",
            "                Train Loss:0.0140 - ACC:0.3978 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1606 - ACC:0.0678 - F1:0.0357 - ROW_F1:0.2591 - MAP:0.1062 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.25641965488018553 -----> 0.2590734588409007\n",
            "Train E:15 - Loss:0.0141: 100% 500/500 [05:10<00:00,  1.61it/s]\n",
            "Valid E:15 - Loss:0.1646: 100% 120/120 [00:31<00:00,  3.81it/s]\n",
            "\n",
            "                Thu Sep 10 06:58:50 2020 \n",
            "\n",
            "                Fold:0, Epoch:15, lr:0.0002926585, \n",
            "\n",
            "                Train Loss:0.0141 - ACC:0.3832 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1646 - ACC:0.0588 - F1:0.0346 - ROW_F1:0.2621 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2590734588409007 -----> 0.2621072689366171\n",
            "Train E:16 - Loss:0.0130: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:16 - Loss:0.1625: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:04:39 2020 \n",
            "\n",
            "                Fold:0, Epoch:16, lr:0.0002916565, \n",
            "\n",
            "                Train Loss:0.0130 - ACC:0.3957 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1625 - ACC:0.0654 - F1:0.0370 - ROW_F1:0.2618 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:17 - Loss:0.0127: 100% 500/500 [05:07<00:00,  1.63it/s]\n",
            "Valid E:17 - Loss:0.1630: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:10:22 2020 \n",
            "\n",
            "                Fold:0, Epoch:17, lr:0.0002905923, \n",
            "\n",
            "                Train Loss:0.0127 - ACC:0.3969 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1630 - ACC:0.0670 - F1:0.0366 - ROW_F1:0.2621 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:18 - Loss:0.0123: 100% 500/500 [05:05<00:00,  1.64it/s]\n",
            "Valid E:18 - Loss:0.1700: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:16:04 2020 \n",
            "\n",
            "                Fold:0, Epoch:18, lr:0.0002894665, \n",
            "\n",
            "                Train Loss:0.0123 - ACC:0.3997 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1700 - ACC:0.0642 - F1:0.0359 - ROW_F1:0.2622 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2621072689366171 -----> 0.26224535845564345\n",
            "Train E:19 - Loss:0.0121: 100% 500/500 [05:10<00:00,  1.61it/s]\n",
            "Valid E:19 - Loss:0.1736: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:21:58 2020 \n",
            "\n",
            "                Fold:0, Epoch:19, lr:0.0002882795, \n",
            "\n",
            "                Train Loss:0.0121 - ACC:0.3883 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1736 - ACC:0.0603 - F1:0.0340 - ROW_F1:0.2637 - MAP:0.1067 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.26224535845564345 -----> 0.2636727860704277\n",
            "Train E:20 - Loss:0.0115: 100% 500/500 [05:02<00:00,  1.65it/s]\n",
            "Valid E:20 - Loss:0.1749: 100% 120/120 [00:31<00:00,  3.81it/s]\n",
            "\n",
            "                Thu Sep 10 07:27:43 2020 \n",
            "\n",
            "                Fold:0, Epoch:20, lr:0.0002870318, \n",
            "\n",
            "                Train Loss:0.0115 - ACC:0.4097 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1749 - ACC:0.0651 - F1:0.0365 - ROW_F1:0.2646 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2636727860704277 -----> 0.26464710745090697\n",
            "Train E:21 - Loss:0.0114: 100% 500/500 [05:09<00:00,  1.62it/s]\n",
            "Valid E:21 - Loss:0.1756: 100% 120/120 [00:31<00:00,  3.79it/s]\n",
            "\n",
            "                Thu Sep 10 07:33:36 2020 \n",
            "\n",
            "                Fold:0, Epoch:21, lr:0.0002857241, \n",
            "\n",
            "                Train Loss:0.0114 - ACC:0.3933 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1756 - ACC:0.0650 - F1:0.0348 - ROW_F1:0.2661 - MAP:0.1070 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.26464710745090697 -----> 0.26614176012145224\n",
            "Train E:22 - Loss:0.0109: 100% 500/500 [05:03<00:00,  1.65it/s]\n",
            "Valid E:22 - Loss:0.1801: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:39:22 2020 \n",
            "\n",
            "                Fold:0, Epoch:22, lr:0.0002843568, \n",
            "\n",
            "                Train Loss:0.0109 - ACC:0.4070 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1801 - ACC:0.0601 - F1:0.0334 - ROW_F1:0.2670 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.26614176012145224 -----> 0.26698438694180593\n",
            "Train E:23 - Loss:0.0108: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:23 - Loss:0.1828: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:45:13 2020 \n",
            "\n",
            "                Fold:0, Epoch:23, lr:0.0002829305, \n",
            "\n",
            "                Train Loss:0.0108 - ACC:0.4008 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1828 - ACC:0.0612 - F1:0.0339 - ROW_F1:0.2667 - MAP:0.1067 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:24 - Loss:0.0105: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:24 - Loss:0.1822: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:50:55 2020 \n",
            "\n",
            "                Fold:0, Epoch:24, lr:0.000281446, \n",
            "\n",
            "                Train Loss:0.0105 - ACC:0.4064 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1822 - ACC:0.0584 - F1:0.0340 - ROW_F1:0.2675 - MAP:0.1071 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.26698438694180593 -----> 0.2674961396284685\n",
            "Train E:25 - Loss:0.0102: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:25 - Loss:0.1811: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 07:56:45 2020 \n",
            "\n",
            "                Fold:0, Epoch:25, lr:0.0002799038, \n",
            "\n",
            "                Train Loss:0.0102 - ACC:0.4042 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1811 - ACC:0.0567 - F1:0.0337 - ROW_F1:0.2680 - MAP:0.1077 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2674961396284685 -----> 0.2680110118072778\n",
            "Train E:26 - Loss:0.0100: 100% 500/500 [05:08<00:00,  1.62it/s]\n",
            "Valid E:26 - Loss:0.1939: 100% 120/120 [00:31<00:00,  3.80it/s]\n",
            "\n",
            "                Thu Sep 10 08:02:35 2020 \n",
            "\n",
            "                Fold:0, Epoch:26, lr:0.0002783046, \n",
            "\n",
            "                Train Loss:0.0100 - ACC:0.4056 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1939 - ACC:0.0609 - F1:0.0331 - ROW_F1:0.2692 - MAP:0.1073 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2680110118072778 -----> 0.2692475265026854\n",
            "Train E:27 - Loss:0.0097: 100% 500/500 [05:05<00:00,  1.64it/s]\n",
            "Valid E:27 - Loss:0.1958: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 08:08:25 2020 \n",
            "\n",
            "                Fold:0, Epoch:27, lr:0.0002766492, \n",
            "\n",
            "                Train Loss:0.0097 - ACC:0.4029 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1958 - ACC:0.0654 - F1:0.0347 - ROW_F1:0.2700 - MAP:0.1070 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2692475265026854 -----> 0.2700199647497388\n",
            "Train E:28 - Loss:0.0096: 100% 500/500 [05:09<00:00,  1.62it/s]\n",
            "Valid E:28 - Loss:0.2012: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 08:14:16 2020 \n",
            "\n",
            "                Fold:0, Epoch:28, lr:0.0002749382, \n",
            "\n",
            "                Train Loss:0.0096 - ACC:0.4007 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2012 - ACC:0.0569 - F1:0.0331 - ROW_F1:0.2702 - MAP:0.1072 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2700199647497388 -----> 0.2701893531733034\n",
            "Train E:29 - Loss:0.0092: 100% 500/500 [05:00<00:00,  1.67it/s]\n",
            "Valid E:29 - Loss:0.1994: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 08:20:00 2020 \n",
            "\n",
            "                Fold:0, Epoch:29, lr:0.0002731724, \n",
            "\n",
            "                Train Loss:0.0092 - ACC:0.4207 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1994 - ACC:0.0589 - F1:0.0339 - ROW_F1:0.2700 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:30 - Loss:0.0094: 100% 500/500 [05:12<00:00,  1.60it/s]\n",
            "Valid E:30 - Loss:0.2044: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 08:25:49 2020 \n",
            "\n",
            "                Fold:0, Epoch:30, lr:0.0002713525, \n",
            "\n",
            "                Train Loss:0.0094 - ACC:0.3962 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2044 - ACC:0.0614 - F1:0.0346 - ROW_F1:0.2713 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2701893531733034 -----> 0.2712610027087591\n",
            "Train E:31 - Loss:0.0095: 100% 500/500 [05:11<00:00,  1.60it/s]\n",
            "Valid E:31 - Loss:0.1937: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 08:31:43 2020 \n",
            "\n",
            "                Fold:0, Epoch:31, lr:0.0002694795, \n",
            "\n",
            "                Train Loss:0.0095 - ACC:0.3947 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1937 - ACC:0.0579 - F1:0.0337 - ROW_F1:0.2701 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:32 - Loss:0.0090: 100% 500/500 [05:12<00:00,  1.60it/s]\n",
            "Valid E:32 - Loss:0.2010: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 08:37:32 2020 \n",
            "\n",
            "                Fold:0, Epoch:32, lr:0.000267554, \n",
            "\n",
            "                Train Loss:0.0090 - ACC:0.3942 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2010 - ACC:0.0582 - F1:0.0339 - ROW_F1:0.2712 - MAP:0.1073 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:33 - Loss:0.0084: 100% 500/500 [04:57<00:00,  1.68it/s]\n",
            "Valid E:33 - Loss:0.1981: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 08:43:06 2020 \n",
            "\n",
            "                Fold:0, Epoch:33, lr:0.000265577, \n",
            "\n",
            "                Train Loss:0.0084 - ACC:0.4329 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.1981 - ACC:0.0615 - F1:0.0341 - ROW_F1:0.2705 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:34 - Loss:0.0084: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:34 - Loss:0.2043: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 08:48:49 2020 \n",
            "\n",
            "                Fold:0, Epoch:34, lr:0.0002635493, \n",
            "\n",
            "                Train Loss:0.0084 - ACC:0.4118 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2043 - ACC:0.0626 - F1:0.0356 - ROW_F1:0.2714 - MAP:0.1067 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2712610027087591 -----> 0.27137528010439904\n",
            "Train E:35 - Loss:0.0083: 100% 500/500 [05:05<00:00,  1.64it/s]\n",
            "Valid E:35 - Loss:0.2099: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 08:54:37 2020 \n",
            "\n",
            "                Fold:0, Epoch:35, lr:0.0002614717, \n",
            "\n",
            "                Train Loss:0.0083 - ACC:0.4166 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2099 - ACC:0.0615 - F1:0.0352 - ROW_F1:0.2726 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27137528010439904 -----> 0.2725829914889856\n",
            "Train E:36 - Loss:0.0082: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:36 - Loss:0.2041: 100% 120/120 [00:31<00:00,  3.80it/s]\n",
            "\n",
            "                Thu Sep 10 09:00:25 2020 \n",
            "\n",
            "                Fold:0, Epoch:36, lr:0.0002593453, \n",
            "\n",
            "                Train Loss:0.0082 - ACC:0.4148 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2041 - ACC:0.0613 - F1:0.0341 - ROW_F1:0.2719 - MAP:0.1070 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:37 - Loss:0.0081: 100% 500/500 [05:03<00:00,  1.65it/s]\n",
            "Valid E:37 - Loss:0.2126: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 09:06:05 2020 \n",
            "\n",
            "                Fold:0, Epoch:37, lr:0.0002571709, \n",
            "\n",
            "                Train Loss:0.0081 - ACC:0.4188 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2126 - ACC:0.0563 - F1:0.0335 - ROW_F1:0.2725 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:38 - Loss:0.0081: 100% 500/500 [05:09<00:00,  1.61it/s]\n",
            "Valid E:38 - Loss:0.2040: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 09:11:52 2020 \n",
            "\n",
            "                Fold:0, Epoch:38, lr:0.0002549495, \n",
            "\n",
            "                Train Loss:0.0081 - ACC:0.4116 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2040 - ACC:0.0654 - F1:0.0368 - ROW_F1:0.2725 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:39 - Loss:0.0077: 100% 500/500 [05:01<00:00,  1.66it/s]\n",
            "Valid E:39 - Loss:0.2072: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 09:17:30 2020 \n",
            "\n",
            "                Fold:0, Epoch:39, lr:0.0002526821, \n",
            "\n",
            "                Train Loss:0.0077 - ACC:0.4260 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2072 - ACC:0.0616 - F1:0.0342 - ROW_F1:0.2719 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:40 - Loss:0.0079: 100% 500/500 [05:10<00:00,  1.61it/s]\n",
            "Valid E:40 - Loss:0.2149: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 09:23:17 2020 \n",
            "\n",
            "                Fold:0, Epoch:40, lr:0.0002503696, \n",
            "\n",
            "                Train Loss:0.0079 - ACC:0.4068 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2149 - ACC:0.0641 - F1:0.0363 - ROW_F1:0.2732 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2725829914889856 -----> 0.27317611092914074\n",
            "Train E:41 - Loss:0.0077: 100% 500/500 [05:10<00:00,  1.61it/s]\n",
            "Valid E:41 - Loss:0.2234: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 09:29:10 2020 \n",
            "\n",
            "                Fold:0, Epoch:41, lr:0.0002480131, \n",
            "\n",
            "                Train Loss:0.0077 - ACC:0.4122 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2234 - ACC:0.0620 - F1:0.0363 - ROW_F1:0.2732 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27317611092914074 -----> 0.2732195238615154\n",
            "Train E:42 - Loss:0.0074: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:42 - Loss:0.2130: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 09:34:56 2020 \n",
            "\n",
            "                Fold:0, Epoch:42, lr:0.0002456136, \n",
            "\n",
            "                Train Loss:0.0074 - ACC:0.4179 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2130 - ACC:0.0603 - F1:0.0355 - ROW_F1:0.2726 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:43 - Loss:0.0074: 100% 500/500 [05:12<00:00,  1.60it/s]\n",
            "Valid E:43 - Loss:0.2163: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 09:40:45 2020 \n",
            "\n",
            "                Fold:0, Epoch:43, lr:0.0002431722, \n",
            "\n",
            "                Train Loss:0.0074 - ACC:0.4057 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2163 - ACC:0.0642 - F1:0.0371 - ROW_F1:0.2735 - MAP:0.1067 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2732195238615154 -----> 0.2734794295488695\n",
            "Train E:44 - Loss:0.0073: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:44 - Loss:0.2139: 100% 120/120 [00:31<00:00,  3.81it/s]\n",
            "\n",
            "                Thu Sep 10 09:46:36 2020 \n",
            "\n",
            "                Fold:0, Epoch:44, lr:0.0002406899, \n",
            "\n",
            "                Train Loss:0.0073 - ACC:0.4180 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2139 - ACC:0.0603 - F1:0.0353 - ROW_F1:0.2730 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:45 - Loss:0.0072: 100% 500/500 [05:09<00:00,  1.61it/s]\n",
            "Valid E:45 - Loss:0.2150: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 09:52:22 2020 \n",
            "\n",
            "                Fold:0, Epoch:45, lr:0.0002381678, \n",
            "\n",
            "                Train Loss:0.0072 - ACC:0.4096 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2150 - ACC:0.0647 - F1:0.0355 - ROW_F1:0.2729 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:46 - Loss:0.0071: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:46 - Loss:0.2151: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 09:58:03 2020 \n",
            "\n",
            "                Fold:0, Epoch:46, lr:0.000235607, \n",
            "\n",
            "                Train Loss:0.0071 - ACC:0.4223 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2151 - ACC:0.0634 - F1:0.0366 - ROW_F1:0.2730 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:47 - Loss:0.0068: 100% 500/500 [05:00<00:00,  1.66it/s]\n",
            "Valid E:47 - Loss:0.2246: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 10:03:40 2020 \n",
            "\n",
            "                Fold:0, Epoch:47, lr:0.0002330087, \n",
            "\n",
            "                Train Loss:0.0068 - ACC:0.4313 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2246 - ACC:0.0633 - F1:0.0362 - ROW_F1:0.2736 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2734794295488695 -----> 0.273568751007336\n",
            "Train E:48 - Loss:0.0069: 100% 500/500 [05:08<00:00,  1.62it/s]\n",
            "Valid E:48 - Loss:0.2159: 100% 120/120 [00:31<00:00,  3.81it/s]\n",
            "\n",
            "                Thu Sep 10 10:09:32 2020 \n",
            "\n",
            "                Fold:0, Epoch:48, lr:0.000230374, \n",
            "\n",
            "                Train Loss:0.0069 - ACC:0.4128 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2159 - ACC:0.0601 - F1:0.0346 - ROW_F1:0.2737 - MAP:0.1070 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.273568751007336 -----> 0.2736500657692928\n",
            "Train E:49 - Loss:0.0069: 100% 500/500 [05:11<00:00,  1.61it/s]\n",
            "Valid E:49 - Loss:0.2293: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 10:15:25 2020 \n",
            "\n",
            "                Fold:0, Epoch:49, lr:0.0002277041, \n",
            "\n",
            "                Train Loss:0.0069 - ACC:0.4087 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2293 - ACC:0.0602 - F1:0.0353 - ROW_F1:0.2737 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2736500657692928 -----> 0.2737014334066414\n",
            "Train E:50 - Loss:0.0068: 100% 500/500 [05:09<00:00,  1.62it/s]\n",
            "Valid E:50 - Loss:0.2315: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 10:21:16 2020 \n",
            "\n",
            "                Fold:0, Epoch:50, lr:0.000225, \n",
            "\n",
            "                Train Loss:0.0068 - ACC:0.4157 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2315 - ACC:0.0629 - F1:0.0355 - ROW_F1:0.2740 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2737014334066414 -----> 0.27403152766729577\n",
            "Train E:51 - Loss:0.0068: 100% 500/500 [05:13<00:00,  1.59it/s]\n",
            "Valid E:51 - Loss:0.2284: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 10:27:12 2020 \n",
            "\n",
            "                Fold:0, Epoch:51, lr:0.0002222631, \n",
            "\n",
            "                Train Loss:0.0068 - ACC:0.4062 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2284 - ACC:0.0689 - F1:0.0378 - ROW_F1:0.2742 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27403152766729577 -----> 0.2741876062577013\n",
            "Train E:52 - Loss:0.0066: 100% 500/500 [05:11<00:00,  1.60it/s]\n",
            "Valid E:52 - Loss:0.2285: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 10:33:08 2020 \n",
            "\n",
            "                Fold:0, Epoch:52, lr:0.0002194944, \n",
            "\n",
            "                Train Loss:0.0066 - ACC:0.4026 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2285 - ACC:0.0561 - F1:0.0341 - ROW_F1:0.2741 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:53 - Loss:0.0064: 100% 500/500 [05:09<00:00,  1.61it/s]\n",
            "Valid E:53 - Loss:0.2196: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 10:38:54 2020 \n",
            "\n",
            "                Fold:0, Epoch:53, lr:0.0002166953, \n",
            "\n",
            "                Train Loss:0.0064 - ACC:0.4096 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2196 - ACC:0.0598 - F1:0.0336 - ROW_F1:0.2738 - MAP:0.1070 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:54 - Loss:0.0063: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:54 - Loss:0.2172: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 10:44:36 2020 \n",
            "\n",
            "                Fold:0, Epoch:54, lr:0.0002138669, \n",
            "\n",
            "                Train Loss:0.0063 - ACC:0.4237 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2172 - ACC:0.0578 - F1:0.0334 - ROW_F1:0.2738 - MAP:0.1067 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:55 - Loss:0.0065: 100% 500/500 [05:14<00:00,  1.59it/s]\n",
            "Valid E:55 - Loss:0.2282: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 10:50:27 2020 \n",
            "\n",
            "                Fold:0, Epoch:55, lr:0.0002110105, \n",
            "\n",
            "                Train Loss:0.0065 - ACC:0.4081 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2282 - ACC:0.0567 - F1:0.0337 - ROW_F1:0.2738 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:56 - Loss:0.0065: 100% 500/500 [05:10<00:00,  1.61it/s]\n",
            "Valid E:56 - Loss:0.2274: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 10:56:14 2020 \n",
            "\n",
            "                Fold:0, Epoch:56, lr:0.0002081273, \n",
            "\n",
            "                Train Loss:0.0065 - ACC:0.4157 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2274 - ACC:0.0599 - F1:0.0352 - ROW_F1:0.2747 - MAP:0.1067 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2741876062577013 -----> 0.27467565080404915\n",
            "Train E:57 - Loss:0.0062: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:57 - Loss:0.2196: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:02:00 2020 \n",
            "\n",
            "                Fold:0, Epoch:57, lr:0.0002052187, \n",
            "\n",
            "                Train Loss:0.0062 - ACC:0.4271 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2196 - ACC:0.0624 - F1:0.0352 - ROW_F1:0.2738 - MAP:0.1070 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:58 - Loss:0.0062: 100% 500/500 [05:11<00:00,  1.60it/s]\n",
            "Valid E:58 - Loss:0.2282: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:07:49 2020 \n",
            "\n",
            "                Fold:0, Epoch:58, lr:0.0002022858, \n",
            "\n",
            "                Train Loss:0.0062 - ACC:0.4129 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2282 - ACC:0.0613 - F1:0.0350 - ROW_F1:0.2746 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:59 - Loss:0.0059: 100% 500/500 [04:56<00:00,  1.68it/s]\n",
            "Valid E:59 - Loss:0.2260: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:13:22 2020 \n",
            "\n",
            "                Fold:0, Epoch:59, lr:0.00019933, \n",
            "\n",
            "                Train Loss:0.0059 - ACC:0.4461 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2260 - ACC:0.0624 - F1:0.0349 - ROW_F1:0.2744 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:60 - Loss:0.0060: 100% 500/500 [05:10<00:00,  1.61it/s]\n",
            "Valid E:60 - Loss:0.2338: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:19:08 2020 \n",
            "\n",
            "                Fold:0, Epoch:60, lr:0.0001963525, \n",
            "\n",
            "                Train Loss:0.0060 - ACC:0.4131 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2338 - ACC:0.0605 - F1:0.0357 - ROW_F1:0.2747 - MAP:0.1065 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27467565080404915 -----> 0.2747471391657438\n",
            "Train E:61 - Loss:0.0060: 100% 500/500 [05:15<00:00,  1.58it/s]\n",
            "Valid E:61 - Loss:0.2399: 100% 120/120 [00:31<00:00,  3.79it/s]\n",
            "\n",
            "                Thu Sep 10 11:25:07 2020 \n",
            "\n",
            "                Fold:0, Epoch:61, lr:0.0001933548, \n",
            "\n",
            "                Train Loss:0.0060 - ACC:0.3965 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2399 - ACC:0.0607 - F1:0.0352 - ROW_F1:0.2747 - MAP:0.1062 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:62 - Loss:0.0059: 100% 500/500 [05:13<00:00,  1.59it/s]\n",
            "Valid E:62 - Loss:0.2361: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:30:57 2020 \n",
            "\n",
            "                Fold:0, Epoch:62, lr:0.000190338, \n",
            "\n",
            "                Train Loss:0.0059 - ACC:0.4112 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2361 - ACC:0.0620 - F1:0.0360 - ROW_F1:0.2749 - MAP:0.1069 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2747471391657438 -----> 0.2749328529315428\n",
            "Train E:63 - Loss:0.0058: 100% 500/500 [05:04<00:00,  1.64it/s]\n",
            "Valid E:63 - Loss:0.2327: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:36:43 2020 \n",
            "\n",
            "                Fold:0, Epoch:63, lr:0.0001873035, \n",
            "\n",
            "                Train Loss:0.0058 - ACC:0.4283 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2327 - ACC:0.0582 - F1:0.0358 - ROW_F1:0.2747 - MAP:0.1072 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:64 - Loss:0.0058: 100% 500/500 [05:09<00:00,  1.61it/s]\n",
            "Valid E:64 - Loss:0.2336: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:42:29 2020 \n",
            "\n",
            "                Fold:0, Epoch:64, lr:0.0001842526, \n",
            "\n",
            "                Train Loss:0.0058 - ACC:0.4203 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2336 - ACC:0.0612 - F1:0.0362 - ROW_F1:0.2749 - MAP:0.1067 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:65 - Loss:0.0057: 100% 500/500 [05:09<00:00,  1.62it/s]\n",
            "Valid E:65 - Loss:0.2424: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 11:48:15 2020 \n",
            "\n",
            "                Fold:0, Epoch:65, lr:0.0001811868, \n",
            "\n",
            "                Train Loss:0.0057 - ACC:0.4163 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2424 - ACC:0.0617 - F1:0.0356 - ROW_F1:0.2751 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2749328529315428 -----> 0.27514425051601604\n",
            "Train E:66 - Loss:0.0057: 100% 500/500 [05:11<00:00,  1.61it/s]\n",
            "Valid E:66 - Loss:0.2457: 100% 120/120 [00:31<00:00,  3.78it/s]\n",
            "\n",
            "                Thu Sep 10 11:54:10 2020 \n",
            "\n",
            "                Fold:0, Epoch:66, lr:0.0001781072, \n",
            "\n",
            "                Train Loss:0.0057 - ACC:0.4124 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2457 - ACC:0.0573 - F1:0.0339 - ROW_F1:0.2752 - MAP:0.1062 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27514425051601604 -----> 0.27524334638320885\n",
            "Train E:67 - Loss:0.0054: 100% 500/500 [05:01<00:00,  1.66it/s]\n",
            "Valid E:67 - Loss:0.2396: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 11:59:55 2020 \n",
            "\n",
            "                Fold:0, Epoch:67, lr:0.0001750153, \n",
            "\n",
            "                Train Loss:0.0054 - ACC:0.4434 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2396 - ACC:0.0638 - F1:0.0362 - ROW_F1:0.2751 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:68 - Loss:0.0054: 100% 500/500 [05:03<00:00,  1.65it/s]\n",
            "Valid E:68 - Loss:0.2437: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 12:05:35 2020 \n",
            "\n",
            "                Fold:0, Epoch:68, lr:0.0001719125, \n",
            "\n",
            "                Train Loss:0.0054 - ACC:0.4330 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2437 - ACC:0.0588 - F1:0.0342 - ROW_F1:0.2752 - MAP:0.1064 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:69 - Loss:0.0055: 100% 500/500 [05:06<00:00,  1.63it/s]\n",
            "Valid E:69 - Loss:0.2422: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 12:11:17 2020 \n",
            "\n",
            "                Fold:0, Epoch:69, lr:0.0001688, \n",
            "\n",
            "                Train Loss:0.0055 - ACC:0.4271 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2422 - ACC:0.0560 - F1:0.0333 - ROW_F1:0.2751 - MAP:0.1064 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:70 - Loss:0.0054: 100% 500/500 [05:00<00:00,  1.67it/s]\n",
            "Valid E:70 - Loss:0.2461: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 12:16:54 2020 \n",
            "\n",
            "                Fold:0, Epoch:70, lr:0.0001656793, \n",
            "\n",
            "                Train Loss:0.0054 - ACC:0.4459 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2461 - ACC:0.0616 - F1:0.0354 - ROW_F1:0.2753 - MAP:0.1064 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27524334638320885 -----> 0.27527906456828827\n",
            "Train E:71 - Loss:0.0053: 100% 500/500 [05:02<00:00,  1.65it/s]\n",
            "Valid E:71 - Loss:0.2394: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 12:22:40 2020 \n",
            "\n",
            "                Fold:0, Epoch:71, lr:0.0001625517, \n",
            "\n",
            "                Train Loss:0.0053 - ACC:0.4328 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2394 - ACC:0.0576 - F1:0.0341 - ROW_F1:0.2752 - MAP:0.1061 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:72 - Loss:0.0053: 100% 500/500 [05:00<00:00,  1.66it/s]\n",
            "Valid E:72 - Loss:0.2352: 100% 120/120 [00:31<00:00,  3.79it/s]\n",
            "\n",
            "                Thu Sep 10 12:28:18 2020 \n",
            "\n",
            "                Fold:0, Epoch:72, lr:0.0001594186, \n",
            "\n",
            "                Train Loss:0.0053 - ACC:0.4396 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2352 - ACC:0.0601 - F1:0.0349 - ROW_F1:0.2751 - MAP:0.1063 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:73 - Loss:0.0053: 100% 500/500 [05:10<00:00,  1.61it/s]\n",
            "Valid E:73 - Loss:0.2435: 100% 120/120 [00:31<00:00,  3.83it/s]\n",
            "\n",
            "                Thu Sep 10 12:34:04 2020 \n",
            "\n",
            "                Fold:0, Epoch:73, lr:0.0001562813, \n",
            "\n",
            "                Train Loss:0.0053 - ACC:0.4166 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2435 - ACC:0.0590 - F1:0.0344 - ROW_F1:0.2754 - MAP:0.1065 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27527906456828827 -----> 0.2753783164100884\n",
            "Train E:74 - Loss:0.0052: 100% 500/500 [05:08<00:00,  1.62it/s]\n",
            "Valid E:74 - Loss:0.2286: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 12:39:55 2020 \n",
            "\n",
            "                Fold:0, Epoch:74, lr:0.0001531414, \n",
            "\n",
            "                Train Loss:0.0052 - ACC:0.4223 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2286 - ACC:0.0585 - F1:0.0346 - ROW_F1:0.2749 - MAP:0.1068 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:75 - Loss:0.0053: 100% 500/500 [05:16<00:00,  1.58it/s]\n",
            "Valid E:75 - Loss:0.2462: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 12:45:48 2020 \n",
            "\n",
            "                Fold:0, Epoch:75, lr:0.00015, \n",
            "\n",
            "                Train Loss:0.0053 - ACC:0.4067 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2462 - ACC:0.0559 - F1:0.0334 - ROW_F1:0.2754 - MAP:0.1062 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.2753783164100884 -----> 0.27543150375118935\n",
            "Train E:76 - Loss:0.0051: 100% 500/500 [05:08<00:00,  1.62it/s]\n",
            "Valid E:76 - Loss:0.2327: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 12:51:39 2020 \n",
            "\n",
            "                Fold:0, Epoch:76, lr:0.0001468586, \n",
            "\n",
            "                Train Loss:0.0051 - ACC:0.4279 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2327 - ACC:0.0607 - F1:0.0362 - ROW_F1:0.2752 - MAP:0.1066 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:77 - Loss:0.0051: 100% 500/500 [05:12<00:00,  1.60it/s]\n",
            "Valid E:77 - Loss:0.2523: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 12:57:28 2020 \n",
            "\n",
            "                Fold:0, Epoch:77, lr:0.0001437187, \n",
            "\n",
            "                Train Loss:0.0051 - ACC:0.4213 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2523 - ACC:0.0605 - F1:0.0356 - ROW_F1:0.2753 - MAP:0.1063 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:78 - Loss:0.0050: 100% 500/500 [05:07<00:00,  1.63it/s]\n",
            "Valid E:78 - Loss:0.2564: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 13:03:11 2020 \n",
            "\n",
            "                Fold:0, Epoch:78, lr:0.0001405814, \n",
            "\n",
            "                Train Loss:0.0050 - ACC:0.4279 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2564 - ACC:0.0569 - F1:0.0340 - ROW_F1:0.2757 - MAP:0.1060 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.27543150375118935 -----> 0.2757115301628896\n",
            "Train E:79 - Loss:0.0050: 100% 500/500 [05:05<00:00,  1.64it/s]\n",
            "Valid E:79 - Loss:0.2534: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 13:09:00 2020 \n",
            "\n",
            "                Fold:0, Epoch:79, lr:0.0001374483, \n",
            "\n",
            "                Train Loss:0.0050 - ACC:0.4334 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2534 - ACC:0.0578 - F1:0.0347 - ROW_F1:0.2756 - MAP:0.1063 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:80 - Loss:0.0049: 100% 500/500 [05:05<00:00,  1.63it/s]\n",
            "Valid E:80 - Loss:0.2478: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 13:14:42 2020 \n",
            "\n",
            "                Fold:0, Epoch:80, lr:0.0001343207, \n",
            "\n",
            "                Train Loss:0.0049 - ACC:0.4308 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2478 - ACC:0.0603 - F1:0.0352 - ROW_F1:0.2754 - MAP:0.1063 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:81 - Loss:0.0049: 100% 500/500 [05:08<00:00,  1.62it/s]\n",
            "Valid E:81 - Loss:0.2492: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 13:20:27 2020 \n",
            "\n",
            "                Fold:0, Epoch:81, lr:0.0001312, \n",
            "\n",
            "                Train Loss:0.0049 - ACC:0.4305 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2492 - ACC:0.0652 - F1:0.0373 - ROW_F1:0.2756 - MAP:0.1061 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:82 - Loss:0.0049: 100% 500/500 [05:12<00:00,  1.60it/s]\n",
            "Valid E:82 - Loss:0.2454: 100% 120/120 [00:31<00:00,  3.79it/s]\n",
            "\n",
            "                Thu Sep 10 13:26:17 2020 \n",
            "\n",
            "                Fold:0, Epoch:82, lr:0.0001280875, \n",
            "\n",
            "                Train Loss:0.0049 - ACC:0.4147 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2454 - ACC:0.0607 - F1:0.0356 - ROW_F1:0.2756 - MAP:0.1061 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:83 - Loss:0.0049: 100% 500/500 [05:12<00:00,  1.60it/s]\n",
            "Valid E:83 - Loss:0.2470: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 13:32:06 2020 \n",
            "\n",
            "                Fold:0, Epoch:83, lr:0.0001249847, \n",
            "\n",
            "                Train Loss:0.0049 - ACC:0.4138 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2470 - ACC:0.0637 - F1:0.0362 - ROW_F1:0.2755 - MAP:0.1058 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:84 - Loss:0.0048: 100% 500/500 [05:07<00:00,  1.63it/s]\n",
            "Valid E:84 - Loss:0.2515: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 13:37:50 2020 \n",
            "\n",
            "                Fold:0, Epoch:84, lr:0.0001218928, \n",
            "\n",
            "                Train Loss:0.0048 - ACC:0.4299 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2515 - ACC:0.0613 - F1:0.0363 - ROW_F1:0.2756 - MAP:0.1058 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:85 - Loss:0.0047: 100% 500/500 [05:03<00:00,  1.65it/s]\n",
            "Valid E:85 - Loss:0.2435: 100% 120/120 [00:31<00:00,  3.82it/s]\n",
            "\n",
            "                Thu Sep 10 13:43:29 2020 \n",
            "\n",
            "                Fold:0, Epoch:85, lr:0.0001188132, \n",
            "\n",
            "                Train Loss:0.0047 - ACC:0.4414 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.2435 - ACC:0.0624 - F1:0.0370 - ROW_F1:0.2755 - MAP:0.1061 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:86 - Loss:0.0046:  27% 136/500 [01:23<03:34,  1.70it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3bRuCBGdH02"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6F86t24-I0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3df9c637-2b7b-4a55-a0e7-4dda84c8c39d"
      },
      "source": [
        "!python3 train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "####################\n",
            "####### FOLD : 0\n",
            ".................... Training started .................\n",
            "Train E:0 - Loss:0.0330: 100% 500/500 [03:49<00:00,  2.18it/s]\n",
            "Valid E:0 - Loss:0.0536: 100% 299/299 [00:39<00:00,  7.57it/s]\n",
            "\n",
            "                Wed Sep  9 18:14:53 2020 \n",
            "\n",
            "                Fold:0, Epoch:0, lr:0.0003, \n",
            "\n",
            "                Train Loss:0.0330 - ACC:0.1754 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0536 - ACC:0.1331 - F1:0.1191 - ROW_F1:0.1287 - MAP:0.0482 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from -99999 -----> 0.12865253430460566\n",
            "Train E:1 - Loss:0.0315: 100% 500/500 [03:53<00:00,  2.14it/s]\n",
            "Valid E:1 - Loss:0.0506: 100% 299/299 [00:39<00:00,  7.49it/s]\n",
            "\n",
            "                Wed Sep  9 18:19:53 2020 \n",
            "\n",
            "                Fold:0, Epoch:1, lr:0.0003, \n",
            "\n",
            "                Train Loss:0.0315 - ACC:0.2166 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0506 - ACC:0.1406 - F1:0.1246 - ROW_F1:0.1349 - MAP:0.0507 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.12865253430460566 -----> 0.1349457153088943\n",
            "Train E:2 - Loss:0.0296: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:2 - Loss:0.0498: 100% 299/299 [00:39<00:00,  7.54it/s]\n",
            "\n",
            "                Wed Sep  9 18:24:50 2020 \n",
            "\n",
            "                Fold:0, Epoch:2, lr:0.0002998684, \n",
            "\n",
            "                Train Loss:0.0296 - ACC:0.2540 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0498 - ACC:0.1413 - F1:0.1252 - ROW_F1:0.1345 - MAP:0.0518 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:3 - Loss:0.0287: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:3 - Loss:0.0485: 100% 299/299 [00:39<00:00,  7.60it/s]\n",
            "\n",
            "                Wed Sep  9 18:29:40 2020 \n",
            "\n",
            "                Fold:0, Epoch:3, lr:0.000299704, \n",
            "\n",
            "                Train Loss:0.0287 - ACC:0.2703 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0485 - ACC:0.1437 - F1:0.1270 - ROW_F1:0.1350 - MAP:0.0528 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.1349457153088943 -----> 0.13496842491418387\n",
            "Train E:4 - Loss:0.0286: 100% 500/500 [03:53<00:00,  2.14it/s]\n",
            "Valid E:4 - Loss:0.0489: 100% 299/299 [00:39<00:00,  7.51it/s]\n",
            "\n",
            "                Wed Sep  9 18:34:40 2020 \n",
            "\n",
            "                Fold:0, Epoch:4, lr:0.0002994739, \n",
            "\n",
            "                Train Loss:0.0286 - ACC:0.2791 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0489 - ACC:0.1464 - F1:0.1290 - ROW_F1:0.1329 - MAP:0.0537 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:5 - Loss:0.0288: 100% 500/500 [03:49<00:00,  2.18it/s]\n",
            "Valid E:5 - Loss:0.0489: 100% 299/299 [00:39<00:00,  7.54it/s]\n",
            "\n",
            "                Wed Sep  9 18:39:29 2020 \n",
            "\n",
            "                Fold:0, Epoch:5, lr:0.0002991783, \n",
            "\n",
            "                Train Loss:0.0288 - ACC:0.2787 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0489 - ACC:0.1417 - F1:0.1251 - ROW_F1:0.1275 - MAP:0.0530 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:6 - Loss:0.0281: 100% 500/500 [03:49<00:00,  2.18it/s]\n",
            "Valid E:6 - Loss:0.0485: 100% 299/299 [00:39<00:00,  7.62it/s]\n",
            "\n",
            "                Wed Sep  9 18:44:18 2020 \n",
            "\n",
            "                Fold:0, Epoch:6, lr:0.0002988172, \n",
            "\n",
            "                Train Loss:0.0281 - ACC:0.2817 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0485 - ACC:0.1466 - F1:0.1296 - ROW_F1:0.1392 - MAP:0.0546 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.13496842491418387 -----> 0.1391893544357973\n",
            "Train E:7 - Loss:0.0282: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:7 - Loss:0.0486: 100% 299/299 [00:39<00:00,  7.52it/s]\n",
            "\n",
            "                Wed Sep  9 18:49:15 2020 \n",
            "\n",
            "                Fold:0, Epoch:7, lr:0.0002983908, \n",
            "\n",
            "                Train Loss:0.0282 - ACC:0.2920 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0486 - ACC:0.1477 - F1:0.1309 - ROW_F1:0.1381 - MAP:0.0551 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:8 - Loss:0.0278: 100% 500/500 [03:49<00:00,  2.18it/s]\n",
            "Valid E:8 - Loss:0.0502: 100% 299/299 [00:39<00:00,  7.52it/s]\n",
            "\n",
            "                Wed Sep  9 18:54:04 2020 \n",
            "\n",
            "                Fold:0, Epoch:8, lr:0.0002978994, \n",
            "\n",
            "                Train Loss:0.0278 - ACC:0.2946 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0502 - ACC:0.1495 - F1:0.1320 - ROW_F1:0.1455 - MAP:0.0557 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.1391893544357973 -----> 0.1454593891116177\n",
            "Train E:9 - Loss:0.0279: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:9 - Loss:0.0497: 100% 299/299 [00:39<00:00,  7.63it/s]\n",
            "\n",
            "                Wed Sep  9 18:59:00 2020 \n",
            "\n",
            "                Fold:0, Epoch:9, lr:0.0002973431, \n",
            "\n",
            "                Train Loss:0.0279 - ACC:0.2995 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0497 - ACC:0.1498 - F1:0.1323 - ROW_F1:0.1447 - MAP:0.0558 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:10 - Loss:0.0279: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:10 - Loss:0.0491: 100% 299/299 [00:39<00:00,  7.49it/s]\n",
            "\n",
            "                Wed Sep  9 19:03:50 2020 \n",
            "\n",
            "                Fold:0, Epoch:10, lr:0.0002967221, \n",
            "\n",
            "                Train Loss:0.0279 - ACC:0.3000 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0491 - ACC:0.1491 - F1:0.1318 - ROW_F1:0.1463 - MAP:0.0561 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.1454593891116177 -----> 0.14629178349011696\n",
            "Train E:11 - Loss:0.0277: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:11 - Loss:0.0502: 100% 299/299 [00:39<00:00,  7.54it/s]\n",
            "\n",
            "                Wed Sep  9 19:08:47 2020 \n",
            "\n",
            "                Fold:0, Epoch:11, lr:0.0002960368, \n",
            "\n",
            "                Train Loss:0.0277 - ACC:0.3070 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0502 - ACC:0.1507 - F1:0.1333 - ROW_F1:0.1472 - MAP:0.0567 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.14629178349011696 -----> 0.14720933888845217\n",
            "Train E:12 - Loss:0.0275: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:12 - Loss:0.0496: 100% 299/299 [00:39<00:00,  7.57it/s]\n",
            "\n",
            "                Wed Sep  9 19:13:44 2020 \n",
            "\n",
            "                Fold:0, Epoch:12, lr:0.0002952875, \n",
            "\n",
            "                Train Loss:0.0275 - ACC:0.3088 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0496 - ACC:0.1505 - F1:0.1327 - ROW_F1:0.1540 - MAP:0.0570 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.14720933888845217 -----> 0.15395103459721018\n",
            "Train E:13 - Loss:0.0269: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:13 - Loss:0.0502: 100% 299/299 [00:39<00:00,  7.62it/s]\n",
            "\n",
            "                Wed Sep  9 19:18:41 2020 \n",
            "\n",
            "                Fold:0, Epoch:13, lr:0.0002944744, \n",
            "\n",
            "                Train Loss:0.0269 - ACC:0.3173 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0502 - ACC:0.1500 - F1:0.1329 - ROW_F1:0.1464 - MAP:0.0570 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:14 - Loss:0.0266: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:14 - Loss:0.0501: 100% 299/299 [00:39<00:00,  7.54it/s]\n",
            "\n",
            "                Wed Sep  9 19:23:31 2020 \n",
            "\n",
            "                Fold:0, Epoch:14, lr:0.0002935979, \n",
            "\n",
            "                Train Loss:0.0266 - ACC:0.3238 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0501 - ACC:0.1513 - F1:0.1338 - ROW_F1:0.1524 - MAP:0.0576 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:15 - Loss:0.0273: 100% 500/500 [03:48<00:00,  2.19it/s]\n",
            "Valid E:15 - Loss:0.0499: 100% 299/299 [00:39<00:00,  7.57it/s]\n",
            "\n",
            "                Wed Sep  9 19:28:19 2020 \n",
            "\n",
            "                Fold:0, Epoch:15, lr:0.0002926585, \n",
            "\n",
            "                Train Loss:0.0273 - ACC:0.3168 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0499 - ACC:0.1521 - F1:0.1344 - ROW_F1:0.1561 - MAP:0.0577 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.15395103459721018 -----> 0.15610276969839892\n",
            "Train E:16 - Loss:0.0268: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:16 - Loss:0.0491: 100% 299/299 [00:39<00:00,  7.49it/s]\n",
            "\n",
            "                Wed Sep  9 19:33:15 2020 \n",
            "\n",
            "                Fold:0, Epoch:16, lr:0.0002916565, \n",
            "\n",
            "                Train Loss:0.0268 - ACC:0.3284 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0491 - ACC:0.1531 - F1:0.1350 - ROW_F1:0.1506 - MAP:0.0583 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:17 - Loss:0.0266: 100% 500/500 [03:48<00:00,  2.18it/s]\n",
            "Valid E:17 - Loss:0.0495: 100% 299/299 [00:39<00:00,  7.52it/s]\n",
            "\n",
            "                Wed Sep  9 19:38:04 2020 \n",
            "\n",
            "                Fold:0, Epoch:17, lr:0.0002905923, \n",
            "\n",
            "                Train Loss:0.0266 - ACC:0.3300 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0495 - ACC:0.1532 - F1:0.1351 - ROW_F1:0.1535 - MAP:0.0584 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:18 - Loss:0.0264: 100% 500/500 [03:48<00:00,  2.19it/s]\n",
            "Valid E:18 - Loss:0.0500: 100% 299/299 [00:39<00:00,  7.54it/s]\n",
            "\n",
            "                Wed Sep  9 19:42:52 2020 \n",
            "\n",
            "                Fold:0, Epoch:18, lr:0.0002894665, \n",
            "\n",
            "                Train Loss:0.0264 - ACC:0.3355 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0500 - ACC:0.1528 - F1:0.1349 - ROW_F1:0.1598 - MAP:0.0590 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.15610276969839892 -----> 0.15977561163081166\n",
            "Train E:19 - Loss:0.0263: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:19 - Loss:0.0492: 100% 299/299 [00:39<00:00,  7.63it/s]\n",
            "\n",
            "                Wed Sep  9 19:47:48 2020 \n",
            "\n",
            "                Fold:0, Epoch:19, lr:0.0002882795, \n",
            "\n",
            "                Train Loss:0.0263 - ACC:0.3375 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0492 - ACC:0.1538 - F1:0.1355 - ROW_F1:0.1526 - MAP:0.0590 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:20 - Loss:0.0268: 100% 500/500 [03:48<00:00,  2.19it/s]\n",
            "Valid E:20 - Loss:0.0496: 100% 299/299 [00:39<00:00,  7.53it/s]\n",
            "\n",
            "                Wed Sep  9 19:52:36 2020 \n",
            "\n",
            "                Fold:0, Epoch:20, lr:0.0002870318, \n",
            "\n",
            "                Train Loss:0.0268 - ACC:0.3333 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0496 - ACC:0.1539 - F1:0.1359 - ROW_F1:0.1620 - MAP:0.0593 \n",
            "\n",
            "\n",
            "        \n",
            "######### >>>>>>> Model Improved from 0.15977561163081166 -----> 0.16202517272401715\n",
            "Train E:21 - Loss:0.0266: 100% 500/500 [03:50<00:00,  2.17it/s]\n",
            "Valid E:21 - Loss:0.0492: 100% 299/299 [00:39<00:00,  7.56it/s]\n",
            "\n",
            "                Wed Sep  9 19:57:32 2020 \n",
            "\n",
            "                Fold:0, Epoch:21, lr:0.0002857241, \n",
            "\n",
            "                Train Loss:0.0266 - ACC:0.3335 - F1:0.0000 - MAP:0.0000  \n",
            "\n",
            "                Valid Loss:0.0492 - ACC:0.1533 - F1:0.1351 - ROW_F1:0.1537 - MAP:0.0591 \n",
            "\n",
            "\n",
            "        \n",
            "Train E:22 - Loss:0.0272:  13% 66/500 [00:31<03:20,  2.16it/s]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}